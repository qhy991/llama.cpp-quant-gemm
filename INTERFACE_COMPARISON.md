# æ¥å£å¯¹æ¯”åˆ†æï¼šæˆ‘ä»¬çš„å®ç° vs llama.cpp

## æ¦‚è¿°

æœ¬æ–‡æ¡£å¯¹æ¯”åˆ†ææˆ‘ä»¬çš„é‡åŒ– GEMM å®ç°ä¸ llama.cpp çš„æ¥å£å·®å¼‚ã€‚

## 1. llama.cpp çš„æ¥å£æ¶æ„

### 1.1 é«˜å±‚æ¥å£ï¼ˆggml.hï¼‰

```cpp
// é€šç”¨çŸ©é˜µä¹˜æ³•æ¥å£
GGML_API struct ggml_tensor * ggml_mul_mat(
    struct ggml_context * ctx,
    struct ggml_tensor  * a,      // æƒé‡çŸ©é˜µ [k, n, ne02, ne03]
    struct ggml_tensor  * b);     // æ¿€æ´»çŸ©é˜µ [k, m, ne02*y, ne03*x]
    // è¿”å›: [m, n, ne02*y, ne03*x]

// ç‰¹ç‚¹ï¼š
// 1. ä½¿ç”¨ ggml_tensor æŠ½è±¡ï¼Œæ”¯æŒå¤šç§æ•°æ®ç±»å‹
// 2. è‡ªåŠ¨å¤„ç†é‡åŒ–ç±»å‹ï¼ˆQ4_0, Q8_0, Q8_1 ç­‰ï¼‰
// 3. æ”¯æŒæ‰¹å¤„ç†å’Œå¹¿æ’­
// 4. é€šè¿‡ backend ç³»ç»Ÿåˆ†å‘åˆ°ä¸åŒç¡¬ä»¶
```

### 1.2 Backend ç³»ç»Ÿ

```
ggml_mul_mat (é«˜å±‚ API)
    â†“
ggml_backend_graph_compute (backend è°ƒåº¦)
    â†“
ggml_cuda_mul_mat (CUDA backend)
    â†“
å…·ä½“çš„ CUDA kernel (mmq, mmvq, dp4a ç­‰)
```

### 1.3 CUDA å®ç°å±‚çº§

llama.cpp çš„ CUDA å®ç°åˆ†ä¸ºå¤šä¸ªå±‚æ¬¡ï¼š

```cpp
// 1. MMQ (Matrix Multiplication Quantized) - ä¸»è¦æ¥å£
// ggml/src/ggml-cuda/mmq.cu
template<typename T>
void ggml_cuda_op_mul_mat_q(
    ggml_backend_cuda_context & ctx,
    const ggml_tensor * src0,  // æƒé‡
    const ggml_tensor * src1,  // æ¿€æ´»
    ggml_tensor * dst,         // è¾“å‡º
    const char * src0_dd_i,    // device data
    const float * src1_ddf_i,
    const char * src1_ddq_i,
    float * dst_dd_i,
    const int64_t row_low,
    const int64_t row_high,
    const int64_t src1_ncols,
    const int64_t src1_padded_row_size,
    cudaStream_t stream);

// 2. Vec Dot - å‘é‡ç‚¹ç§¯ï¼ˆç”¨äº batch=1ï¼‰
// ggml/src/ggml-cuda/vecdotq.cuh
template <int vdr>
static __device__ __forceinline__ float vec_dot_q4_0_q8_1_impl(
    const int * v,      // Q4_0 æƒé‡
    const int * u,      // Q8_1 æ¿€æ´»
    const float & d4,   // Q4_0 scale
    const half2 & ds8); // Q8_1 scale + sum
```

## 2. æˆ‘ä»¬çš„æ¥å£æ¶æ„

### 2.1 å½“å‰å®ç°

```cpp
// Host wrapper å‡½æ•°
inline void gemm_w4a8_dp4a(
    const block_q8_1* A,  // æ¿€æ´»çŸ©é˜µï¼ˆé‡åŒ–åï¼‰
    const block_q4_0* B,  // æƒé‡çŸ©é˜µï¼ˆé‡åŒ–åï¼‰
    float* C,             // è¾“å‡ºçŸ©é˜µï¼ˆFP32ï¼‰
    int M, int N, int K,  // çŸ©é˜µç»´åº¦
    cudaStream_t stream = 0);

// CUDA kernel
__global__ void gemm_w4a8_dp4a_kernel(
    const block_q8_1* __restrict__ A,
    const block_q4_0* __restrict__ B,
    float* __restrict__ C,
    int M, int N, int K);
```

### 2.2 æˆ‘ä»¬çš„å®ç°å±‚çº§

```
ç”¨æˆ·ä»£ç 
    â†“
gemm_w4a8_dp4a (host wrapper)
    â†“
gemm_w4a8_dp4a_kernel (CUDA kernel)
    â†“
dp4a, load_int_b2/b4 (device å‡½æ•°)
```

## 3. å…³é”®å·®å¼‚å¯¹æ¯”

### 3.1 æ¥å£æŠ½è±¡å±‚æ¬¡

| æ–¹é¢ | llama.cpp | æˆ‘ä»¬çš„å®ç° | å·®å¼‚ |
|------|-----------|-----------|------|
| **æŠ½è±¡çº§åˆ«** | é«˜å±‚ï¼ˆggml_tensorï¼‰ | ä½å±‚ï¼ˆåŸå§‹æŒ‡é’ˆï¼‰ | âš ï¸ ä¸åŒ |
| **ç±»å‹ç³»ç»Ÿ** | ç»Ÿä¸€çš„ tensor ç±»å‹ | ç‰¹å®šçš„é‡åŒ–ç±»å‹ | âš ï¸ ä¸åŒ |
| **Backend æŠ½è±¡** | æ”¯æŒå¤š backend | ä»… CUDA | âš ï¸ ä¸åŒ |
| **é‡åŒ–å¤„ç†** | å†…éƒ¨è‡ªåŠ¨å¤„ç† | å¤–éƒ¨é¢„é‡åŒ– | âš ï¸ ä¸åŒ |

### 3.2 æ•°æ®æ ¼å¼å…¼å®¹æ€§

| æ–¹é¢ | llama.cpp | æˆ‘ä»¬çš„å®ç° | å…¼å®¹æ€§ |
|------|-----------|-----------|--------|
| **block_q4_0** | 18 å­—èŠ‚ | 18 å­—èŠ‚ | âœ… å®Œå…¨å…¼å®¹ |
| **block_q8_0** | 34 å­—èŠ‚ | 34 å­—èŠ‚ | âœ… å®Œå…¨å…¼å®¹ |
| **block_q8_1** | 36 å­—èŠ‚ | 36 å­—èŠ‚ | âœ… å®Œå…¨å…¼å®¹ |
| **å†…å­˜å¸ƒå±€** | ç›¸åŒ | ç›¸åŒ | âœ… å®Œå…¨å…¼å®¹ |
| **è¡¥å¿å…¬å¼** | ç›¸åŒ | ç›¸åŒ | âœ… å®Œå…¨å…¼å®¹ |

### 3.3 è®¡ç®—é€»è¾‘å…¼å®¹æ€§

| æ–¹é¢ | llama.cpp | æˆ‘ä»¬çš„å®ç° | å…¼å®¹æ€§ |
|------|-----------|-----------|--------|
| **DP4A æŒ‡ä»¤** | ä½¿ç”¨ | ä½¿ç”¨ | âœ… ç›¸åŒ |
| **å†…å­˜åŠ è½½** | load_int_b2/b4 | load_int_b2/b4 | âœ… ç›¸åŒ |
| **è¡¥å¿å…¬å¼** | d_w Ã— (d_a Ã— sumi - 8 Ã— s_a) | ç›¸åŒ | âœ… ç›¸åŒ |
| **æ•°å€¼ç²¾åº¦** | NMSE < 1e-13 | NMSE < 1e-13 | âœ… ç›¸åŒ |

### 3.4 åŠŸèƒ½å¯¹æ¯”

| åŠŸèƒ½ | llama.cpp | æˆ‘ä»¬çš„å®ç° | çŠ¶æ€ |
|------|-----------|-----------|------|
| **åŸºç¡€ GEMM** | âœ… | âœ… | å®Œå…¨æ”¯æŒ |
| **æ‰¹å¤„ç†** | âœ… | âœ… | æ”¯æŒï¼ˆé€šè¿‡ M ç»´åº¦ï¼‰|
| **å¹¿æ’­** | âœ… | âŒ | ä¸æ”¯æŒ |
| **Tensor è§†å›¾** | âœ… | âŒ | ä¸æ”¯æŒ |
| **å¤š Backend** | âœ… | âŒ | ä»… CUDA |
| **è‡ªåŠ¨é‡åŒ–** | âœ… | âŒ | éœ€æ‰‹åŠ¨é‡åŒ– |
| **æ¢¯åº¦è®¡ç®—** | âœ… | âŒ | ä¸æ”¯æŒ |

## 4. æ¥å£å…¼å®¹æ€§åˆ†æ

### 4.1 æ•°æ®å±‚å…¼å®¹ âœ…

**ç»“è®ºï¼šå®Œå…¨å…¼å®¹**

æˆ‘ä»¬çš„é‡åŒ–æ•°æ®ç»“æ„ä¸ llama.cpp 100% å…¼å®¹ï¼š

```cpp
// å¯ä»¥ç›´æ¥è¯»å– llama.cpp çš„é‡åŒ–æƒé‡
FILE* f = fopen("llama_weights.bin", "rb");
block_q4_0* weights = new block_q4_0[n_blocks];
fread(weights, sizeof(block_q4_0), n_blocks, f);

// ç›´æ¥ä½¿ç”¨æˆ‘ä»¬çš„ kernel
gemm_w4a8_dp4a(activations, weights, output, M, N, K);
```

### 4.2 è®¡ç®—å±‚å…¼å®¹ âœ…

**ç»“è®ºï¼šå®Œå…¨å…¼å®¹**

æˆ‘ä»¬çš„è®¡ç®—é€»è¾‘ä¸ llama.cpp çš„ `vec_dot_q4_0_q8_1_impl` å®Œå…¨ä¸€è‡´ï¼š

```cpp
// llama.cpp (vecdotq.cuh:102-121)
template <int vdr>
static __device__ __forceinline__ float vec_dot_q4_0_q8_1_impl(
    const int * v, const int * u, const float & d4, const half2 & ds8) {
    int sumi = 0;
    for (int i = 0; i < vdr; ++i) {
        const int vi0 = (v[i] >> 0) & 0x0F0F0F0F;
        const int vi1 = (v[i] >> 4) & 0x0F0F0F0F;
        sumi = ggml_cuda_dp4a(vi0, u[2*i+0], sumi);
        sumi = ggml_cuda_dp4a(vi1, u[2*i+1], sumi);
    }
    const float2 ds8f = __half22float2(ds8);
    return d4 * (sumi * ds8f.x - (8*vdr/QI4_0) * ds8f.y);
}

// æˆ‘ä»¬çš„å®ç° (gemm_cuda_dp4a.cuh:118-148)
// å®Œå…¨ç›¸åŒçš„é€»è¾‘ï¼Œåªæ˜¯å°è£…åœ¨ GEMM kernel ä¸­
```

### 4.3 API å±‚ä¸å…¼å®¹ âš ï¸

**ç»“è®ºï¼šæ¥å£ä¸åŒï¼Œä½†å¯ä»¥æ¡¥æ¥**

| å±‚æ¬¡ | llama.cpp | æˆ‘ä»¬çš„å®ç° | æ¡¥æ¥éš¾åº¦ |
|------|-----------|-----------|---------|
| **é«˜å±‚ API** | `ggml_mul_mat(ctx, a, b)` | `gemm_w4a8_dp4a(A, B, C, M, N, K)` | ğŸŸ¡ ä¸­ç­‰ |
| **æ•°æ®æŠ½è±¡** | `ggml_tensor*` | `block_q4_0*`, `block_q8_1*` | ğŸŸ¢ ç®€å• |
| **Backend** | å¤š backend | CUDA only | ğŸ”´ å›°éš¾ |

## 5. å¦‚ä½•æ¡¥æ¥åˆ° llama.cpp æ¥å£

### 5.1 æ–¹æ¡ˆ 1ï¼šåˆ›å»º GGML Backendï¼ˆæ¨èï¼‰

```cpp
// åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰ GGML backend
struct ggml_backend_custom_context {
    // æˆ‘ä»¬çš„å®ç°
};

static void ggml_backend_custom_mul_mat(
    ggml_backend_t backend,
    struct ggml_tensor * dst,
    struct ggml_tensor * src0,
    struct ggml_tensor * src1) {

    // æå–æ•°æ®
    const block_q4_0* weights = (const block_q4_0*)src0->data;
    const block_q8_1* acts = (const block_q8_1*)src1->data;
    float* output = (float*)dst->data;

    // æå–ç»´åº¦
    int M = src1->ne[1];
    int N = src0->ne[1];
    int K = src0->ne[0];

    // è°ƒç”¨æˆ‘ä»¬çš„å®ç°
    gemm_w4a8_dp4a(acts, weights, output, M, N, K);
}

// æ³¨å†Œ backend
ggml_backend_t backend = ggml_backend_custom_init();
```

### 5.2 æ–¹æ¡ˆ 2ï¼šç›´æ¥æ›¿æ¢ llama.cpp çš„ kernel

```cpp
// åœ¨ llama.cpp/ggml/src/ggml-cuda/mmq.cu ä¸­
// æ›¿æ¢ç°æœ‰çš„ kernel è°ƒç”¨ä¸ºæˆ‘ä»¬çš„å®ç°

void ggml_cuda_op_mul_mat_q(...) {
    // ... åŸæœ‰ä»£ç  ...

    // æ›¿æ¢ä¸ºæˆ‘ä»¬çš„ kernel
    gemm_w4a8_dp4a_kernel<<<grid, block>>>(
        (const block_q8_1*)src1_ddq_i,
        (const block_q4_0*)src0_dd_i,
        dst_dd_i,
        M, N, K);
}
```

### 5.3 æ–¹æ¡ˆ 3ï¼šåˆ›å»º Wrapper å±‚

```cpp
// åˆ›å»ºä¸€ä¸ªå…¼å®¹å±‚
class QuantGEMMWrapper {
public:
    // llama.cpp é£æ ¼çš„æ¥å£
    static void mul_mat(
        ggml_tensor* dst,
        const ggml_tensor* src0,
        const ggml_tensor* src1) {

        // ç±»å‹æ£€æŸ¥
        assert(src0->type == GGML_TYPE_Q4_0);
        assert(src1->type == GGML_TYPE_Q8_1);

        // è°ƒç”¨æˆ‘ä»¬çš„å®ç°
        gemm_w4a8_dp4a(
            (const block_q8_1*)src1->data,
            (const block_q4_0*)src0->data,
            (float*)dst->data,
            src1->ne[1], src0->ne[1], src0->ne[0]);
    }
};
```

## 6. test-backend-ops.cpp çš„æµ‹è¯•è¦æ±‚

### 6.1 æµ‹è¯•æ¡†æ¶è¦æ±‚

```cpp
struct test_mul_mat : public test_case {
    // å¿…é¡»å®ç°çš„æ¥å£ï¼š
    ggml_tensor * build_graph(ggml_context * ctx) override;
    double max_nmse_err() override;  // æœ€å¤§å…è®¸è¯¯å·®
    uint64_t op_flops(ggml_tensor * t) override;  // FLOPS è®¡ç®—
    std::string op_desc(ggml_tensor * t) override;  // æ“ä½œæè¿°
};
```

### 6.2 æˆ‘ä»¬éœ€è¦å®ç°çš„å†…å®¹

è¦é€šè¿‡ `test-backend-ops.cpp` çš„æµ‹è¯•ï¼Œæˆ‘ä»¬éœ€è¦ï¼š

1. **å®ç° ggml_tensor æ¥å£** âš ï¸
   - å½“å‰ï¼šä½¿ç”¨åŸå§‹æŒ‡é’ˆ
   - éœ€è¦ï¼šæ”¯æŒ `ggml_tensor*`

2. **å®ç° ggml_context ç®¡ç†** âš ï¸
   - å½“å‰ï¼šæ‰‹åŠ¨å†…å­˜ç®¡ç†
   - éœ€è¦ï¼šä½¿ç”¨ GGML çš„å†…å­˜åˆ†é…å™¨

3. **å®ç° backend æ¥å£** âš ï¸
   - å½“å‰ï¼šç›´æ¥è°ƒç”¨ CUDA kernel
   - éœ€è¦ï¼šé€šè¿‡ `ggml_backend` ç³»ç»Ÿ

4. **æ”¯æŒå¤šç§é…ç½®** âš ï¸
   - æ‰¹å¤„ç†ã€å¹¿æ’­ã€è§†å›¾ç­‰

## 7. æ€»ç»“

### 7.1 å…¼å®¹æ€§çŸ©é˜µ

| å±‚æ¬¡ | å…¼å®¹æ€§ | è¯´æ˜ |
|------|--------|------|
| **æ•°æ®æ ¼å¼** | âœ… 100% | å®Œå…¨å…¼å®¹ llama.cpp |
| **è®¡ç®—é€»è¾‘** | âœ… 100% | æ•°å€¼ç»“æœå®Œå…¨ä¸€è‡´ |
| **Kernel æ¥å£** | âœ… 90% | å¯ç›´æ¥æ›¿æ¢ llama.cpp çš„ kernel |
| **Host æ¥å£** | âš ï¸ 50% | éœ€è¦ wrapper å±‚ |
| **GGML API** | âŒ 0% | éœ€è¦å®Œæ•´å®ç° backend |

### 7.2 å½“å‰çŠ¶æ€

**æˆ‘ä»¬çš„å®ç°æ˜¯ llama.cpp çš„"å†…æ ¸çº§"å…¼å®¹å®ç°**ï¼š

- âœ… **æ•°æ®å±‚**ï¼š100% å…¼å®¹ï¼Œå¯ä»¥ç›´æ¥è¯»å– llama.cpp çš„æƒé‡
- âœ… **è®¡ç®—å±‚**ï¼š100% å…¼å®¹ï¼Œæ•°å€¼ç»“æœå®Œå…¨ä¸€è‡´
- âœ… **Kernel å±‚**ï¼š90% å…¼å®¹ï¼Œå¯ä»¥ç›´æ¥æ›¿æ¢ llama.cpp çš„ kernel
- âš ï¸ **API å±‚**ï¼šéœ€è¦æ¡¥æ¥å±‚æ‰èƒ½ä¸ `ggml_mul_mat` æ¥å£å¯¹æ¥
- âŒ **æ¡†æ¶å±‚**ï¼šä¸å…¼å®¹ `test-backend-ops.cpp` çš„æµ‹è¯•æ¡†æ¶

### 7.3 æ¨èä½¿ç”¨åœºæ™¯

#### åœºæ™¯ 1ï¼šå­¦ä¹ å’Œç ”ç©¶ âœ…
- **å½“å‰å®ç°å®Œç¾é€‚ç”¨**
- ä»£ç æ¸…æ™°ï¼Œæ˜“äºç†è§£
- å¯ä»¥ç›´æ¥çœ‹åˆ°é‡åŒ– GEMM çš„æ ¸å¿ƒé€»è¾‘

#### åœºæ™¯ 2ï¼šæ›¿æ¢ llama.cpp çš„ kernel âœ…
- **å¯ä»¥ç›´æ¥ä½¿ç”¨**
- åœ¨ `mmq.cu` ä¸­æ›¿æ¢ kernel è°ƒç”¨
- ä¿æŒæ•°æ®æ ¼å¼ä¸å˜

#### åœºæ™¯ 3ï¼šç‹¬ç«‹ä½¿ç”¨ï¼ˆä¸ä¾èµ– GGMLï¼‰âœ…
- **å¯ä»¥ç›´æ¥ä½¿ç”¨**
- é€‚åˆè‡ªå®šä¹‰æ¨ç†å¼•æ“
- éœ€è¦è‡ªå·±å¤„ç†é‡åŒ–

#### åœºæ™¯ 4ï¼šé›†æˆåˆ° GGML æ¡†æ¶ âš ï¸
- **éœ€è¦é¢å¤–å·¥ä½œ**
- å®ç° backend æ¥å£
- å®ç° tensor æŠ½è±¡

#### åœºæ™¯ 5ï¼šé€šè¿‡ test-backend-ops.cpp æµ‹è¯• âŒ
- **éœ€è¦å¤§é‡é¢å¤–å·¥ä½œ**
- å®ç°å®Œæ•´çš„ GGML backend
- ä¸æ¨èï¼ˆé™¤éè¦è´¡çŒ®åˆ° llama.cppï¼‰

### 7.4 ç»“è®º

**æˆ‘ä»¬çš„å®ç°ä¸ llama.cpp åœ¨æ ¸å¿ƒå±‚é¢ï¼ˆæ•°æ®æ ¼å¼ã€è®¡ç®—é€»è¾‘ï¼‰æ˜¯å®Œå…¨å…¼å®¹çš„ï¼Œä½†åœ¨ API å±‚é¢æ˜¯ä¸åŒçš„ã€‚**

- å¦‚æœç›®æ ‡æ˜¯**å­¦ä¹ é‡åŒ– GEMM**ï¼šå½“å‰å®ç°å·²ç»å®Œç¾ âœ…
- å¦‚æœç›®æ ‡æ˜¯**æ›¿æ¢ llama.cpp çš„ kernel**ï¼šå¯ä»¥ç›´æ¥ä½¿ç”¨ âœ…
- å¦‚æœç›®æ ‡æ˜¯**é€šè¿‡ test-backend-ops.cpp**ï¼šéœ€è¦å®ç° GGML backend âš ï¸

å¯¹äºæ•™å­¦é¡¹ç›®æ¥è¯´ï¼Œå½“å‰çš„æ¥å£è®¾è®¡æ›´åŠ æ¸…æ™°å’Œç›´è§‚ï¼Œæ›´é€‚åˆç†è§£é‡åŒ– GEMM çš„æ ¸å¿ƒåŸç†ã€‚
