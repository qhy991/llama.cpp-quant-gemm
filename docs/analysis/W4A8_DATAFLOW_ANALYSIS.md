# W4A8 数据流分析

## 1. 当前实现分析

### 1.1 现有 API 的数据类型

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        当前 Python API                                  │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  用户输入:                                                              │
│    weight:     FP32 [M, K]                                              │
│    activation: FP32 [N, K]                                              │
│                                                                         │
│  Step 1: 离线量化 (用户手动调用)                                        │
│    weight_q     = quantize_q4_0(weight)      # FP32 → Q4_0             │
│    activation_q = quantize_q8_1(activation)  # FP32 → Q8_1             │
│                                                                         │
│  Step 2: 计算                                                           │
│    output = gemm_q4_0_q8_1(weight_q, activation_q, M, N, K)            │
│                                                                         │
│  内部数据流:                                                            │
│    Q4_0 [M, K/32, 18 bytes] × Q8_1 [N, K/32, 36 bytes] → FP32 [M, N]   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 1.2 数据格式详解

| 格式 | 每块元素数 | 每块字节数 | 内容 |
|------|-----------|-----------|------|
| **Q4_0** | 32 | 18 | `half d` (2B) + `uint8 qs[16]` (16B) |
| **Q8_1** | 32 | 36 | `half2 ds` (4B) + `int8 qs[32]` (32B) |

**Q4_0 量化公式**:
```
d = max(|x|) / 7.0
q = round(x / d) + 8    # q ∈ [0, 15]
x_dequant = (q - 8) * d
```

**Q8_1 量化公式**:
```
d = max(|x|) / 127.0
s = sum(x)              # 原始值的和
q = round(x / d)        # q ∈ [-127, 127]
```

---

## 2. 你希望的 W4A8 流程

### 2.1 目标数据流

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        W4A8 推理流程                                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  离线阶段 (模型加载时):                                                 │
│    weight_q4 = load_quantized_weights()  # 预量化的 Q4_0 权重          │
│                                                                         │
│  在线阶段 (每次推理):                                                   │
│    activation_fp32 = model_input         # FP32 激活 [N, K]            │
│                                          ↓                              │
│                              ┌───────────────────────┐                  │
│                              │  在线量化 (融合到kernel)│                 │
│                              │  FP32 → Q8_1           │                  │
│                              └───────────────────────┘                  │
│                                          ↓                              │
│                              ┌───────────────────────┐                  │
│                              │  INT8 矩阵乘法         │                  │
│                              │  Q4_0 × Q8_1 (DP4A)   │                  │
│                              └───────────────────────┘                  │
│                                          ↓                              │
│    output_fp32 = result                  # FP32 输出 [M, N]            │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 2.2 与现有实现的区别

| 方面 | 当前实现 | W4A8 目标 |
|------|---------|----------|
| **权重输入** | FP32 (需要手动量化) | Q4_0 (预量化) |
| **激活输入** | FP32 (需要手动量化) | FP32 (直接传入) |
| **激活量化** | 离线 (用户调用) | **在线 (kernel 内部)** |
| **API 调用** | 2 步 (量化 + GEMM) | **1 步 (直接 GEMM)** |

---

## 3. 需要新增的 API

### 3.1 新 API 设计

```python
def gemm_w4a8(
    weight_q4: torch.Tensor,    # Q4_0 预量化权重 [M, K//32, 18], uint8
    activation: torch.Tensor,   # FP32 激活 [N, K], float32
    M: int, N: int, K: int
) -> torch.Tensor:              # FP32 输出 [M, N]
    """
    W4A8 量化 GEMM: 权重 4-bit，激活在线量化为 8-bit

    内部流程:
    1. 将 FP32 activation 在线量化为 Q8_1
    2. 执行 Q4_0 × Q8_1 GEMM
    3. 返回 FP32 结果
    """
    pass
```

### 3.2 使用示例

```python
import torch
import quant_gemm

# ==================== 离线阶段 ====================
# 模型加载时，权重已经是 Q4_0 格式
weight_fp32 = torch.randn(4096, 14336, device='cuda')
weight_q4 = quant_gemm.quantize_q4_0(weight_fp32)  # 离线量化一次
# 实际使用中，weight_q4 直接从模型文件加载

# ==================== 在线阶段 ====================
# 每次推理时，激活是 FP32
activation = torch.randn(2, 14336, device='cuda')  # FP32 输入

# 方式 1: 当前 API (需要手动量化激活)
activation_q8 = quant_gemm.quantize_q8_1(activation)  # 额外的量化调用
output = quant_gemm.gemm_q4_0_q8_1(weight_q4, activation_q8, 4096, 2, 14336)

# 方式 2: 新 API (激活在线量化，融合到 kernel)
output = quant_gemm.gemm_w4a8(weight_q4, activation, 4096, 2, 14336)  # 一步完成
```

---

## 4. 实现方案

### 4.1 方案 A: Python 层封装 (简单)

在 Python 层组合现有函数：

```python
def gemm_w4a8(weight_q4, activation_fp32, M, N, K):
    """简单封装：Python 层组合"""
    activation_q8 = quantize_q8_1(activation_fp32)
    return gemm_q4_0_q8_1(weight_q4, activation_q8, M, N, K)
```

**优点**: 无需修改 CUDA 代码
**缺点**: 需要额外的 kernel launch 和显存分配

### 4.2 方案 B: 融合 Kernel (优化)

创建一个融合的 CUDA kernel：

```cpp
// 融合 kernel: 量化 + GEMM 一次完成
__global__ void gemm_w4a8_fused_kernel(
    const block_q4_0* __restrict__ weight,   // Q4_0 权重
    const float* __restrict__ activation,     // FP32 激活
    float* __restrict__ output,
    int M, int N, int K
) {
    // 1. 每个线程块负责一部分激活的量化
    __shared__ block_q8_1 shared_activation[...];

    // 2. 在线量化 activation 到 shared memory
    // ...

    // 3. 执行 GEMM
    // ...
}
```

**优点**:
- 减少 kernel launch 开销
- 避免中间结果的显存分配
- 更好的 cache 利用率

**缺点**: 实现复杂

### 4.3 推荐方案

**先用方案 A 快速实现**，验证正确性后再优化为方案 B。

---

## 5. 数据流对比图

```
┌─────────────────────────────────────────────────────────────────────────┐
│                     当前 vs W4A8 数据流对比                              │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  【当前实现】                                                            │
│                                                                         │
│    weight_fp32 ──┬──→ quantize_q4_0 ──→ weight_q4 ──┐                  │
│                  │                                   │                  │
│                  │                                   ↓                  │
│                  │                            ┌──────────────┐          │
│                  │                            │ gemm_q4_0_q8_1│ → output│
│                  │                            └──────────────┘          │
│                  │                                   ↑                  │
│    act_fp32 ─────┴──→ quantize_q8_1 ──→ act_q8 ─────┘                  │
│                                                                         │
│    kernel launches: 3 (quant_q4 + quant_q8 + gemm)                     │
│    显存分配: weight_q4 + act_q8 + output                                │
│                                                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  【W4A8 目标 (融合)】                                                   │
│                                                                         │
│    weight_q4 (预加载) ──────────────────────┐                          │
│                                              ↓                          │
│                                       ┌──────────────┐                  │
│    act_fp32 ─────────────────────────→│  gemm_w4a8   │ → output        │
│                                       │ (内部量化)    │                  │
│                                       └──────────────┘                  │
│                                                                         │
│    kernel launches: 1                                                   │
│    显存分配: weight_q4 (预加载) + output (无中间结果)                    │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 6. 完整类型总结

### 6.1 输入类型

| 输入 | 当前实现 | W4A8 目标 |
|------|---------|----------|
| **权重** | `FP32 [M, K]` → 手动量化 | `Q4_0 [M, K/32, 18]` 预量化 |
| **激活** | `FP32 [N, K]` → 手动量化 | `FP32 [N, K]` 直接传入 |

### 6.2 内部计算类型

| 阶段 | 数据类型 |
|------|---------|
| 权重存储 | Q4_0 (4-bit) |
| 激活存储 | FP32 → Q8_1 (8-bit) |
| **乘法计算** | **INT8 × INT8 (DP4A 指令)** |
| 累加器 | INT32 |
| 输出 | FP32 |

### 6.3 DP4A 计算示例

```
Q4_0 权重:  [q0, q1, q2, q3]  每个 4-bit, 存储为 [0,15], 实际值 = (q-8)*scale
Q8_1 激活:  [a0, a1, a2, a3]  每个 8-bit, 有符号 [-127, 127]

DP4A 指令 (一条指令完成 4 个乘加):
  result = q0*a0 + q1*a1 + q2*a2 + q3*a3  (INT32 累加)

最终反量化:
  output = scale_w * (scale_a * sum_int - 8 * sum_a)
```

---

## 7. 下一步行动

1. **快速实现**: 在 Python 层添加 `gemm_w4a8` 封装
2. **验证正确性**: 与 FP32 参考对比
3. **性能测试**: 与 llama.cpp 对比
4. **优化**: 如果需要，实现融合 kernel

要我现在实现这个新的 `gemm_w4a8` API 吗？
