# é›†æˆæŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•å°† quant-gemm-from-scratch çš„ kernel é›†æˆåˆ° llama.cpp æˆ–å…¶ä»–é¡¹ç›®ä¸­ã€‚

---

## ğŸ“‹ ç›®å½•

- [é›†æˆæ–¹æ³•](#é›†æˆæ–¹æ³•)
- [åµŒå…¥å¼é›†æˆ](#åµŒå…¥å¼é›†æˆ)
- [ç‹¬ç«‹åº“é›†æˆ](#ç‹¬ç«‹åº“é›†æˆ)
- [ç«¯åˆ°ç«¯éªŒè¯](#ç«¯åˆ°ç«¯éªŒè¯)
- [æ€§èƒ½å¯¹æ¯”](#æ€§èƒ½å¯¹æ¯”)

---

## ğŸ¯ é›†æˆæ–¹æ³•

### æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **åµŒå…¥å¼é›†æˆ** | ç±»å‹å…¼å®¹ä¿è¯ | éœ€è¦ä¿®æ”¹æºç  | å¼€å‘æµ‹è¯• |
| **ç‹¬ç«‹åº“é›†æˆ** | ä¸ä¿®æ”¹æºç  | éœ€è¦é€‚é…å±‚ | ç”Ÿäº§ç¯å¢ƒ |
| **æ›¿æ¢å¼é›†æˆ** | å®Œå…¨æ›¿æ¢åŸç‰ˆ | é£é™©è¾ƒé«˜ | æ€§èƒ½ä¼˜åŒ– |

---

## ğŸ”Œ åµŒå…¥å¼é›†æˆ

### æ­¥éª¤ 1: ä¿®æ”¹ llama.cpp

```bash
cd /home/haiyan/Agent4Kernel/llama.cpp
```

ç¼–è¾‘ `ggml/src/ggml-cuda/mmq.cuh`ï¼Œåœ¨ç¬¬13è¡Œæ·»åŠ ï¼š

```cuda
// ============================================================================
// CUSTOM KERNEL: Include our custom gemm_w4a8_dp4a implementation
// ============================================================================
#include "/home/haiyan/Agent4Kernel/quant-gemm-from-scratch/include/gemm_cuda_dp4a.cuh"
```

### æ­¥éª¤ 2: ç¼–è¯‘ llama.cpp

```bash
mkdir -p build && cd build
cmake .. -DLLAMA_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=86
make -j$(nproc)
```

### æ­¥éª¤ 3: éªŒè¯é›†æˆ

```bash
# è¿è¡Œå¯¹æ¯”æµ‹è¯•
cd /home/haiyan/Agent4Kernel/llama.cpp/tests
nvcc -o mmq_vs_baseline_test mmq_vs_baseline_test.cu \
  -I../ggml/include -I../ggml/src \
  -I/home/haiyan/Agent4Kernel/quant-gemm-from-scratch/include \
  -lcuda -lcudart

./mmq_vs_baseline_test
```

**é¢„æœŸè¾“å‡º**:
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   llama.cpp MMQ vs Baseline å¯¹æ¯”æµ‹è¯•        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[æµ‹è¯•: M1_K4096_N4096]
  Baseline:       2.3456 ms
    vs CPU:       âœ“ PASS (max_err: 1.23e-05)
  MMQ DP4A:       0.5678 ms
    vs CPU:       âœ“ PASS (max_err: 1.45e-05)
  Baseline vs MMQ: âœ“ PASS (max_err: 2.34e-06)
  åŠ é€Ÿæ¯”: 4.13x
```

---

## ğŸ“¦ ç‹¬ç«‹åº“é›†æˆ

### æ–¹æ³• 1: é™æ€åº“

#### ç¼–è¯‘é™æ€åº“

```bash
cd /home/haiyan/Agent4Kernel/quant-gemm-from-scratch
mkdir -p build && cd build

cmake .. -DBUILD_SHARED_LIBS=OFF
make -j

# ç”Ÿæˆ libquant_gemm.a
```

#### ä½¿ç”¨é™æ€åº“

```cpp
// your_project.cu
#include <quant_gemm/gemm_cuda_dp4a.cuh>

int main() {
    // ä½¿ç”¨ kernel
    gemm_w4a8_dp4a(A, B, C, M, N, K);
    return 0;
}
```

```bash
# ç¼–è¯‘ä½ çš„é¡¹ç›®
nvcc -o your_project your_project.cu \
  -I/path/to/quant-gemm-from-scratch/include \
  -L/path/to/quant-gemm-from-scratch/build \
  -lquant_gemm
```

### æ–¹æ³• 2: Header-Only

å°†æ‰€æœ‰å®ç°æ”¾åœ¨ `.cuh` æ–‡ä»¶ä¸­ï¼Œç›´æ¥ includeï¼š

```cpp
#include "quant-gemm-from-scratch/include/gemm_cuda_dp4a.cuh"
```

**ä¼˜ç‚¹**: æ— éœ€é“¾æ¥åº“
**ç¼ºç‚¹**: ç¼–è¯‘æ—¶é—´è¾ƒé•¿

---

## ğŸ”„ æ›¿æ¢å¼é›†æˆ

### å®Œå…¨æ›¿æ¢ llama.cpp çš„ vec_dot å®ç°

#### æ­¥éª¤ 1: å¤‡ä»½åŸç‰ˆ

```bash
cd /home/haiyan/Agent4Kernel/llama.cpp/ggml/src/ggml-cuda
cp vecdotq.cuh vecdotq.cuh.backup
```

#### æ­¥éª¤ 2: ä¿®æ”¹ vecdotq.cuh

```cuda
// vecdotq.cuh

// åŸç‰ˆå®ç°
#if 0  // ç¦ç”¨åŸç‰ˆ
template <int vdr> static __device__ __forceinline__ float vec_dot_q4_0_q8_1_impl(
    const int * v, const int * u, const float & d4, const half2 & ds8) {
    // ... åŸç‰ˆä»£ç 
}
#endif

// ä½¿ç”¨è‡ªå®šä¹‰å®ç°
#include "/path/to/quant-gemm-from-scratch/kernels/gemm/gemm_quant_formats.cuh"

// é€‚é…å±‚
template <int vdr> static __device__ __forceinline__ float vec_dot_q4_0_q8_1_impl(
    const int * v, const int * u, const float & d4, const half2 & ds8) {

    // è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
    block_q4_0 bq4;
    block_q8_1 bq8;
    // ... æ•°æ®è½¬æ¢

    // è°ƒç”¨æˆ‘ä»¬çš„å®ç°
    return vec_dot_q4_0_q8_1(&bq4, &bq8);
}
```

#### æ­¥éª¤ 3: æµ‹è¯•éªŒè¯

```bash
# ç¼–è¯‘
cd build
make -j

# è¿è¡Œæµ‹è¯•
./bin/llama-cli -m model-Q4_0.gguf -p "Hello" -n 50
```

---

## âœ… ç«¯åˆ°ç«¯éªŒè¯

### çœŸå®æ¨¡å‹æ¨ç†æµ‹è¯•

#### å‡†å¤‡æ¨¡å‹

```bash
# ä¸‹è½½æˆ–é‡åŒ–æ¨¡å‹
cd /home/haiyan/Agent4Kernel/llama.cpp
./bin/llama-quantize model.gguf model-Q4_0.gguf Q4_0
./bin/llama-quantize model.gguf model-Q4_1.gguf Q4_1
./bin/llama-quantize model.gguf model-Q5_0.gguf Q5_0
./bin/llama-quantize model.gguf model-Q5_1.gguf Q5_1
```

#### è¿è¡Œæ¨ç†å¯¹æ¯”

```bash
# 1. ä½¿ç”¨åŸç‰ˆ llama.cpp
./bin/llama-cli-original -m model-Q4_0.gguf -p "Hello, world!" -n 50 \
  --seed 42 > original_output.txt

# 2. ä½¿ç”¨è‡ªå®šä¹‰ kernel
./bin/llama-cli-custom -m model-Q4_0.gguf -p "Hello, world!" -n 50 \
  --seed 42 > custom_output.txt

# 3. å¯¹æ¯”è¾“å‡º
diff original_output.txt custom_output.txt
```

#### æ•°å€¼å¯¹æ¯”

```python
# compare_outputs.py
import numpy as np

def compare_outputs(file1, file2):
    with open(file1) as f1, open(file2) as f2:
        lines1 = f1.readlines()
        lines2 = f2.readlines()

    # æå– token æ¦‚ç‡
    probs1 = extract_probs(lines1)
    probs2 = extract_probs(lines2)

    # è®¡ç®—å·®å¼‚
    diff = np.abs(probs1 - probs2)
    print(f"Max diff: {diff.max():.6f}")
    print(f"Mean diff: {diff.mean():.6f}")

    return diff.max() < 1e-4  # é˜ˆå€¼

if __name__ == "__main__":
    result = compare_outputs("original_output.txt", "custom_output.txt")
    print("âœ“ PASS" if result else "âœ— FAIL")
```

---

## âš¡ æ€§èƒ½å¯¹æ¯”

### Benchmark æµ‹è¯•

```bash
# åˆ›å»º benchmark è„šæœ¬
cat > benchmark.sh << 'EOF'
#!/bin/bash

MODEL="model-Q4_0.gguf"
PROMPT="Once upon a time"
N_TOKENS=100

echo "=== Original llama.cpp ==="
time ./bin/llama-cli-original -m $MODEL -p "$PROMPT" -n $N_TOKENS

echo ""
echo "=== Custom kernel ==="
time ./bin/llama-cli-custom -m $MODEL -p "$PROMPT" -n $N_TOKENS
EOF

chmod +x benchmark.sh
./benchmark.sh
```

### ä½¿ç”¨ Nsight Systems åˆ†æ

```bash
# åˆ†æåŸç‰ˆ
nsys profile -o original ./bin/llama-cli-original -m model-Q4_0.gguf -p "Hello" -n 50

# åˆ†æè‡ªå®šä¹‰ç‰ˆæœ¬
nsys profile -o custom ./bin/llama-cli-custom -m model-Q4_0.gguf -p "Hello" -n 50

# å¯¹æ¯”
nsys-ui original.nsys-rep custom.nsys-rep
```

---

## ğŸ” é›†æˆéªŒè¯æ¸…å•

### ç¼–è¯‘æ—¶éªŒè¯

- [ ] æ— ç¼–è¯‘é”™è¯¯
- [ ] æ— é“¾æ¥é”™è¯¯
- [ ] æ— ç±»å‹ä¸åŒ¹é…è­¦å‘Š

### åŠŸèƒ½éªŒè¯

- [ ] å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] è¾“å‡ºæ•°å€¼æ­£ç¡®ï¼ˆè¯¯å·® < 1%ï¼‰

### æ€§èƒ½éªŒè¯

- [ ] æ€§èƒ½ä¸ä½äºåŸç‰ˆ
- [ ] å†…å­˜ä½¿ç”¨åˆç†
- [ ] æ— å†…å­˜æ³„æ¼

### ç«¯åˆ°ç«¯éªŒè¯

- [ ] çœŸå®æ¨¡å‹æ¨ç†æˆåŠŸ
- [ ] è¾“å‡ºä¸åŸç‰ˆä¸€è‡´
- [ ] æ— å´©æºƒæˆ–é”™è¯¯

---

## ğŸ› å¸¸è§é›†æˆé—®é¢˜

### é—®é¢˜ 1: ç±»å‹ä¸åŒ¹é…

**é”™è¯¯**:
```
error: no matching function for call to 'vec_dot_q4_0_q8_1'
```

**è§£å†³**:
```cuda
// ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„ç±»å‹å®šä¹‰
#include "ggml-cuda/common.cuh"  // llama.cpp çš„ç±»å‹

// æˆ–è€…ä½¿ç”¨æ¡ä»¶ç¼–è¯‘
#ifdef GGML_COMMON_DECL
    // ä½¿ç”¨ llama.cpp çš„ç±»å‹
#else
    // ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„ç±»å‹
    #include "quant_types.h"
#endif
```

### é—®é¢˜ 2: ç¬¦å·é‡å®šä¹‰

**é”™è¯¯**:
```
multiple definition of `gemm_w4a8_dp4a_kernel'
```

**è§£å†³**:
```cuda
// ä½¿ç”¨ static å…³é”®å­—
static __global__ void gemm_w4a8_dp4a_kernel(...)
```

### é—®é¢˜ 3: è·¯å¾„é—®é¢˜

**é”™è¯¯**:
```
fatal error: gemm_cuda_dp4a.cuh: No such file or directory
```

**è§£å†³**:
```bash
# ä½¿ç”¨ç»å¯¹è·¯å¾„
#include "/home/haiyan/Agent4Kernel/quant-gemm-from-scratch/include/gemm_cuda_dp4a.cuh"

# æˆ–è€…æ·»åŠ åˆ° include è·¯å¾„
nvcc -I/home/haiyan/Agent4Kernel/quant-gemm-from-scratch/include ...
```

---

## ğŸ“Š é›†æˆæµ‹è¯•æŠ¥å‘Š

æŸ¥çœ‹è¯¦ç»†çš„é›†æˆæµ‹è¯•ç»“æœï¼š

- [é›†æˆæµ‹è¯•æŠ¥å‘Š](../reports/INTEGRATION_TEST_REPORT.md)
- [æœ€ç»ˆæµ‹è¯•æŠ¥å‘Š](../reports/FINAL_TEST_REPORT.md)

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [æµ‹è¯•æŒ‡å—](TESTING_GUIDE.md) - å¦‚ä½•è¿è¡Œæµ‹è¯•
- [æµ‹è¯•æ–¹æ³•åˆ†æ](../analysis/TESTING_METHOD_ANALYSIS.md) - æµ‹è¯•æ–¹æ³•è®º
- [æ¥å£å¯¹é½](../INTERFACE_ALIGNMENT_STATUS.md) - æ¥å£å…¼å®¹æ€§

---

**æœ€åæ›´æ–°**: 2026-01-29
