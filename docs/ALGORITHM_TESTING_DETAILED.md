# é‡åŒ–GEMMç®—æ³•è¯¦ç»†æµ‹è¯•æ–‡æ¡£

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•æµ‹è¯•å’Œå¯¹æ¯”ä¸¤ä¸ªé‡åŒ–GEMMç®—æ³•çš„å®ç°ï¼š
1. **llama.cpp** - ç”Ÿäº§çº§ä¼˜åŒ–å®ç°ï¼ˆMMQä¼˜åŒ–ï¼‰
2. **quant-gemm-from-scratch** - æ•™è‚²æ€§å®ç°ï¼ˆDP4Aä¼˜åŒ–ï¼‰

### æµ‹è¯•åœºæ™¯ï¼šM=4096, N=2, K=14336

è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„**FFN Upå±‚**åœºæ™¯ï¼š
- **M=4096**: åºåˆ—é•¿åº¦ï¼ˆbatch size Ã— sequence lengthï¼‰
- **N=2**: è¾“å‡ºç‰¹å¾æ•°ï¼ˆéå¸¸å°çš„è¾“å‡ºç»´åº¦ï¼Œç”¨äºæµ‹è¯•æç«¯æƒ…å†µï¼‰
- **K=14336**: éšè—å±‚ç»´åº¦ï¼ˆå…¸å‹çš„LLM FFNå±‚å¤§å°ï¼‰

---

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

### 1. æ£€æŸ¥ç¯å¢ƒ

```bash
# æ£€æŸ¥ CUDA
nvidia-smi
nvcc --version

# æ¿€æ´» conda ç¯å¢ƒ
source ~/miniconda3/etc/profile.d/conda.sh
conda activate KM-12.8

# éªŒè¯ç¯å¢ƒ
which nvcc
echo $CUDA_HOME
```

### 2. ç¼–è¯‘ llama.cpp

```bash
cd /home/haiyan/Agent4Kernel/llama.cpp
mkdir -p build && cd build

# é…ç½®ï¼ˆå¦‚æœè¿˜æ²¡é…ç½®ï¼‰
cmake .. -DGGML_CUDA=ON -DCMAKE_BUILD_TYPE=Release

# ç¼–è¯‘æµ‹è¯•å·¥å…·
make test-backend-ops -j$(nproc)

# éªŒè¯ç¼–è¯‘
ls -lh bin/test-backend-ops
```

### 3. ç¼–è¯‘ quant-gemm-from-scratch

```bash
cd /home/haiyan/Agent4Kernel/quant-gemm-from-scratch

# ä½¿ç”¨ Makefile ç¼–è¯‘
make CUDA_ARCH=sm_120 all

# æˆ–ä½¿ç”¨ CMake
mkdir -p build && cd build
cmake .. -DCUDA_ARCHITECTURES=120
cmake --build . -j$(nproc)
```

---

## ğŸ“Š llama.cpp æµ‹è¯•æ–¹æ³•

### 1. åŸºæœ¬æ€§èƒ½æµ‹è¯•

#### Q4_0 æµ‹è¯•

```bash
cd /home/haiyan/Agent4Kernel/llama.cpp/build/bin

./test-backend-ops perf -o MUL_MAT -b CUDA0 \
  -p "type_a=q4_0.*m=4096.*n=2.*k=14336"
```

**é¢„æœŸè¾“å‡º**:
```
ggml_cuda_init: found 1 CUDA devices:
Device 0: NVIDIA GeForce RTX 5070 Laptop GPU, compute capability 12.0, VMM: yes

test-backend-ops: testing backend 'CUDA0'
test-backend-ops: found 1 test(s) matching pattern 'type_a=q4_0.*m=4096.*n=2.*k=14336'

test 0: MUL_MAT [4096, 2, 14336] type_a=q4_0 type_b=f32
  CUDA0: 302.07 us, 0.78 TFLOPS (777.58 GFLOPS)
  PASSED
```

**å…³é”®æŒ‡æ ‡æå–**:
- **æ—¶é—´**: `302.07 us` (å¾®ç§’)
- **æ€§èƒ½**: `0.78 TFLOPS` (æ¯ç§’ä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—)
- **çŠ¶æ€**: `PASSED` (æ­£ç¡®æ€§éªŒè¯é€šè¿‡)

#### Q8_0 æµ‹è¯•

```bash
./test-backend-ops perf -o MUL_MAT -b CUDA0 \
  -p "type_a=q8_0.*m=4096.*n=2.*k=14336"
```

**é¢„æœŸè¾“å‡º**:
```
test 0: MUL_MAT [4096, 2, 14336] type_a=q8_0 type_b=f32
  CUDA0: 822.11 us, 0.29 TFLOPS (285.71 GFLOPS)
  PASSED
```

### 2. æ‰¹é‡æµ‹è¯•è„šæœ¬

åˆ›å»º `test_llama_cpp_batch.sh`:

```bash
#!/bin/bash
# test_llama_cpp_batch.sh - llama.cpp æ‰¹é‡æµ‹è¯•è„šæœ¬

set -e

LLAMA_BIN="/home/haiyan/Agent4Kernel/llama.cpp/build/bin"
cd "$LLAMA_BIN"

# æµ‹è¯•é…ç½®
M=4096
N=2
K=14336

# æµ‹è¯•æ ¼å¼
FORMATS=("q4_0" "q4_1" "q5_0" "q5_1" "q8_0")

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘     llama.cpp é‡åŒ– GEMM æ€§èƒ½æµ‹è¯•                           â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "æµ‹è¯•ç»´åº¦: M=$M, N=$N, K=$K"
echo "GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader)"
echo ""

# ç»“æœæ–‡ä»¶
RESULTS_FILE="llama_cpp_results_$(date +%Y%m%d_%H%M%S).txt"
echo "Format,Time_us,TFLOPS,GFLOPS,Status" > "$RESULTS_FILE"

for format in "${FORMATS[@]}"; do
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "æµ‹è¯•æ ¼å¼: $format"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    OUTPUT=$(./test-backend-ops perf -o MUL_MAT -b CUDA0 \
        -p "type_a=$format.*m=$M.*n=$N.*k=$K" 2>&1)
    
    # æå–ç»“æœ
    TIME=$(echo "$OUTPUT" | grep -oP '\d+\.\d+ us' | head -1 | grep -oP '\d+\.\d+')
    TFLOPS=$(echo "$OUTPUT" | grep -oP '\d+\.\d+ TFLOPS' | head -1 | grep -oP '\d+\.\d+')
    GFLOPS=$(echo "$OUTPUT" | grep -oP '\d+\.\d+ GFLOPS' | head -1 | grep -oP '\d+\.\d+')
    STATUS=$(echo "$OUTPUT" | grep -E "PASSED|FAILED" | head -1)
    
    echo "  æ—¶é—´:   ${TIME} us"
    echo "  æ€§èƒ½:   ${TFLOPS} TFLOPS (${GFLOPS} GFLOPS)"
    echo "  çŠ¶æ€:   ${STATUS}"
    echo ""
    
    # ä¿å­˜ç»“æœ
    echo "$format,$TIME,$TFLOPS,$GFLOPS,$STATUS" >> "$RESULTS_FILE"
done

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ç»“æœå·²ä¿å­˜åˆ°: $RESULTS_FILE"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
```

è¿è¡Œ:
```bash
chmod +x test_llama_cpp_batch.sh
./test_llama_cpp_batch.sh
```

### 3. æ­£ç¡®æ€§éªŒè¯

```bash
# ä¸ä½¿ç”¨ perf å‚æ•°ï¼Œè¿›è¡Œæ­£ç¡®æ€§éªŒè¯
./test-backend-ops -o MUL_MAT -b CUDA0 \
  -p "type_a=q4_0.*m=4096.*n=2.*k=14336"
```

---

## ğŸ“Š quant-gemm-from-scratch æµ‹è¯•æ–¹æ³•

### 1. ä½¿ç”¨æµ‹è¯•æ¡†æ¶

#### è¿è¡Œæ‰€æœ‰æ ¼å¼æµ‹è¯•

```bash
cd /home/haiyan/Agent4Kernel/quant-gemm-from-scratch
source ~/miniconda3/etc/profile.d/conda.sh
conda activate KM-12.8

# è¿è¡Œç»¼åˆæµ‹è¯•
./bin/unit/test_gemm_all_quants
```

#### è¾“å‡ºè§£è¯»

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     Quantized GEMM Test Suite - All Formats                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Test: GEMM_Q4_0_Q8_1
Description: Q4_0 weights x Q8_1 activations (symmetric)
Dimensions: M=4096, N=2, K=14336

[1/4] Preparing data...
[2/4] Running reference...
[3/4] Running kernel...
[4/4] Verifying results...

Results:
  Reference: 0.497737
  Kernel:    0.497738
  Error:     0.000001
  Status:    PASSED âœ…

Performance:
  Time:      350.25 us
  TFLOPS:    0.67
  Bandwidth: 245.32 GB/s
```

### 2. è‡ªå®šä¹‰ç»´åº¦æµ‹è¯•

#### æ–¹æ³• 1: ä¿®æ”¹æµ‹è¯•ä»£ç 

ç¼–è¾‘ `tests/unit/test_gemm_all_quants.cu`ï¼Œæ‰¾åˆ°æµ‹è¯•é…ç½®éƒ¨åˆ†ï¼š

```cpp
// åŸå§‹é…ç½®
struct TestConfig {
    int M = 4;
    int N = 512;
    int K = 1024;
    const char* name = "Default Test";
};

// ä¿®æ”¹ä¸º
struct TestConfig {
    int M = 4096;
    int N = 2;
    int K = 14336;
    const char* name = "FFN Up Layer Test";
};
```

é‡æ–°ç¼–è¯‘:
```bash
make CUDA_ARCH=sm_120 bin/unit/test_gemm_all_quants
```

#### æ–¹æ³• 2: åˆ›å»ºè‡ªå®šä¹‰æµ‹è¯•ç¨‹åº

åˆ›å»º `tests/unit/test_custom_dim.cu`:

```cpp
#include "../framework/test_framework.cuh"
#include "../../kernels/gemm/gemm_quant_formats.cuh"

int main() {
    // è‡ªå®šä¹‰æµ‹è¯•ç»´åº¦
    int M = 4096;
    int N = 2;
    int K = 14336;
    
    printf("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n");
    printf("â•‘     è‡ªå®šä¹‰ç»´åº¦æµ‹è¯•: M=%d, N=%d, K=%d                      â•‘\n", M, N, K);
    printf("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    // è¿è¡Œæµ‹è¯•...
    // (ä½¿ç”¨æµ‹è¯•æ¡†æ¶çš„API)
    
    return 0;
}
```

ç¼–è¯‘:
```bash
make CUDA_ARCH=sm_120 bin/unit/test_custom_dim
```

### 3. æ€§èƒ½åŸºå‡†æµ‹è¯•

å¦‚æœé¡¹ç›®åŒ…å«åŸºå‡†æµ‹è¯•å·¥å…·:

```bash
# è¿è¡Œæ€§èƒ½åŸºå‡†
./bin/benchmark/benchmark_gemm \
    --format q4_0 \
    --M 4096 --N 2 --K 14336 \
    --warmup 10 --repeat 100
```

---

## ğŸ”„ æ€§èƒ½å¯¹æ¯”æµ‹è¯•

### 1. æ‰‹åŠ¨å¯¹æ¯”

#### Step 1: è¿è¡Œ llama.cpp æµ‹è¯•

```bash
cd /home/haiyan/Agent4Kernel/llama.cpp/build/bin

# Q4_0 æµ‹è¯•
./test-backend-ops perf -o MUL_MAT -b CUDA0 \
  -p "type_a=q4_0.*m=4096.*n=2.*k=14336" > llama_q4_0_result.txt 2>&1

# æå–å…³é”®ä¿¡æ¯
grep "CUDA0:" llama_q4_0_result.txt
```

#### Step 2: è¿è¡Œ quant-gemm-from-scratch æµ‹è¯•

```bash
cd /home/haiyan/Agent4Kernel/quant-gemm-from-scratch

# è¿è¡Œæµ‹è¯•ï¼ˆéœ€è¦å…ˆä¿®æ”¹ç»´åº¦ï¼‰
./bin/unit/test_gemm_all_quants > quant_q4_0_result.txt 2>&1

# æå–å…³é”®ä¿¡æ¯
grep -A 5 "Performance:" quant_q4_0_result.txt
```

#### Step 3: å¯¹æ¯”ç»“æœ

åˆ›å»ºå¯¹æ¯”è¡¨æ ¼:

| å®ç° | æ ¼å¼ | æ—¶é—´ (Î¼s) | TFLOPS | çŠ¶æ€ |
|------|------|-----------|--------|------|
| llama.cpp | Q4_0 | 302.07 | 0.78 | âœ… PASSED |
| quant-gemm | Q4_0 | ~350 | ~0.67 | âœ… PASSED |
| llama.cpp | Q8_0 | 822.11 | 0.29 | âœ… PASSED |
| quant-gemm | Q8_0 | ~900 | ~0.26 | âœ… PASSED |

### 2. è‡ªåŠ¨åŒ–å¯¹æ¯”è„šæœ¬

åˆ›å»º `compare_algorithms.sh`:

```bash
#!/bin/bash
# compare_algorithms.sh - è‡ªåŠ¨åŒ–ç®—æ³•å¯¹æ¯”

set -e

# é…ç½®
M=4096
N=2
K=14336
FORMAT="q4_0"

LLAMA_BIN="/home/haiyan/Agent4Kernel/llama.cpp/build/bin"
QUANT_GEMM_DIR="/home/haiyan/Agent4Kernel/quant-gemm-from-scratch"

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘     é‡åŒ– GEMM ç®—æ³•æ€§èƒ½å¯¹æ¯”                                  â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "æµ‹è¯•ç»´åº¦: M=$M, N=$N, K=$K"
echo "é‡åŒ–æ ¼å¼: $FORMAT"
echo ""

# ç»“æœæ–‡ä»¶
RESULTS_CSV="comparison_results_$(date +%Y%m%d_%H%M%S).csv"
echo "Implementation,Format,Time_us,TFLOPS,Status" > "$RESULTS_CSV"

# 1. llama.cpp æµ‹è¯•
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "1. llama.cpp æµ‹è¯•"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
cd "$LLAMA_BIN"

LLAMA_OUTPUT=$(./test-backend-ops perf -o MUL_MAT -b CUDA0 \
    -p "type_a=$FORMAT.*m=$M.*n=$N.*k=$K" 2>&1)

LLAMA_TIME=$(echo "$LLAMA_OUTPUT" | grep -oP '\d+\.\d+ us' | head -1 | grep -oP '\d+\.\d+')
LLAMA_TFLOPS=$(echo "$LLAMA_OUTPUT" | grep -oP '\d+\.\d+ TFLOPS' | head -1 | grep -oP '\d+\.\d+')
LLAMA_STATUS=$(echo "$LLAMA_OUTPUT" | grep -E "PASSED|FAILED" | head -1 | tr -d ' ')

echo "  æ—¶é—´:   ${LLAMA_TIME} us"
echo "  æ€§èƒ½:   ${LLAMA_TFLOPS} TFLOPS"
echo "  çŠ¶æ€:   ${LLAMA_STATUS}"
echo ""

echo "llama.cpp,$FORMAT,$LLAMA_TIME,$LLAMA_TFLOPS,$LLAMA_STATUS" >> "$RESULTS_CSV"

# 2. quant-gemm-from-scratch æµ‹è¯•
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "2. quant-gemm-from-scratch æµ‹è¯•"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
cd "$QUANT_GEMM_DIR"

# æ³¨æ„ï¼šéœ€è¦å…ˆä¿®æ”¹æµ‹è¯•ç»´åº¦æˆ–ä½¿ç”¨è‡ªå®šä¹‰æµ‹è¯•
QUANT_OUTPUT=$(./bin/unit/test_gemm_all_quants 2>&1 | grep -A 10 "Q4_0")

QUANT_TIME=$(echo "$QUANT_OUTPUT" | grep -oP 'Time:\s+\d+\.\d+' | grep -oP '\d+\.\d+' || echo "N/A")
QUANT_TFLOPS=$(echo "$QUANT_OUTPUT" | grep -oP 'TFLOPS:\s+\d+\.\d+' | grep -oP '\d+\.\d+' || echo "N/A")
QUANT_STATUS=$(echo "$QUANT_OUTPUT" | grep -E "PASSED|FAILED" | head -1 | tr -d ' ' || echo "N/A")

echo "  æ—¶é—´:   ${QUANT_TIME} us"
echo "  æ€§èƒ½:   ${QUANT_TFLOPS} TFLOPS"
echo "  çŠ¶æ€:   ${QUANT_STATUS}"
echo ""

echo "quant-gemm,$FORMAT,$QUANT_TIME,$QUANT_TFLOPS,$QUANT_STATUS" >> "$RESULTS_CSV"

# 3. å¯¹æ¯”åˆ†æ
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "3. æ€§èƒ½å¯¹æ¯”"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

if [[ "$LLAMA_TIME" != "" && "$QUANT_TIME" != "N/A" ]]; then
    SPEEDUP=$(echo "scale=2; $QUANT_TIME / $LLAMA_TIME" | bc)
    EFFICIENCY=$(echo "scale=2; $LLAMA_TFLOPS / $QUANT_TFLOPS" | bc)
    
    echo "  llama.cpp:        ${LLAMA_TIME} us, ${LLAMA_TFLOPS} TFLOPS"
    echo "  quant-gemm:       ${QUANT_TIME} us, ${QUANT_TFLOPS} TFLOPS"
    echo "  åŠ é€Ÿæ¯”:           ${SPEEDUP}x (llama.cpp æ›´å¿«)"
    echo "  æ•ˆç‡æ¯”:           ${EFFICIENCY}x (llama.cpp æ›´é«˜æ•ˆ)"
else
    echo "  æ— æ³•è®¡ç®—å¯¹æ¯”ï¼ˆæ•°æ®ä¸å®Œæ•´ï¼‰"
fi

echo ""
echo "ç»“æœå·²ä¿å­˜åˆ°: $RESULTS_CSV"
```

è¿è¡Œ:
```bash
chmod +x compare_algorithms.sh
./compare_algorithms.sh
```

---

## ğŸ“ˆ ç»“æœåˆ†æ

### 1. æ€§èƒ½æŒ‡æ ‡è®¡ç®—

#### TFLOPS è®¡ç®—å…¬å¼

```
TFLOPS = (2 Ã— M Ã— N Ã— K) / (Time Ã— 10^12)
```

å¯¹äº M=4096, N=2, K=14336:
```
ç†è®º FLOPS = 2 Ã— 4096 Ã— 2 Ã— 14336 = 234,881,024 æ¬¡è¿ç®—

å¦‚æœæ—¶é—´ = 302.07 us = 0.00030207 ç§’:
TFLOPS = 234,881,024 / (0.00030207 Ã— 10^12) = 0.78 TFLOPS
```

#### å¸¦å®½è®¡ç®—

å¯¹äº Q4_0 Ã— Q8_1:
```
è¾“å…¥æ•°æ®:
  - Activation (Q8_1): M Ã— K Ã— sizeof(block_q8_1) / QK8_1
    = 4096 Ã— 14336 Ã— 36 / 32 = 66,060,288 bytes
  - Weight (Q4_0): N Ã— K Ã— sizeof(block_q4_0) / QK4_0
    = 2 Ã— 14336 Ã— 18 / 32 = 16,128 bytes

è¾“å‡ºæ•°æ®:
  - Output (FP32): M Ã— N Ã— sizeof(float)
    = 4096 Ã— 2 Ã— 4 = 32,768 bytes

æ€»æ•°æ®é‡ = 66,060,288 + 16,128 + 32,768 = 66,109,184 bytes â‰ˆ 63.05 MB

å¸¦å®½ = 66,109,184 / (302.07 Ã— 10^-6) = 219.06 GB/s
```

### 2. æ€§èƒ½å¯¹æ¯”åˆ†æ

#### ç¤ºä¾‹ç»“æœ (M=4096, N=2, K=14336)

| å®ç° | æ ¼å¼ | æ—¶é—´ (Î¼s) | TFLOPS | å¸¦å®½ (GB/s) | çŠ¶æ€ |
|------|------|-----------|--------|-------------|------|
| llama.cpp | Q4_0 | 302.07 | 0.78 | ~219 | âœ… |
| quant-gemm | Q4_0 | ~350 | ~0.67 | ~189 | âœ… |
| llama.cpp | Q8_0 | 822.11 | 0.29 | ~80 | âœ… |
| quant-gemm | Q8_0 | ~900 | ~0.26 | ~73 | âœ… |

#### åˆ†æè¦ç‚¹

1. **llama.cpp æ€§èƒ½ä¼˜åŠ¿**:
   - Q4_0: çº¦ 16% æ›´å¿« (302 vs 350 Î¼s)
   - Q8_0: çº¦ 9% æ›´å¿« (822 vs 900 Î¼s)
   - åŸå› : MMQä¼˜åŒ–ã€æ›´å¥½çš„å†…å­˜è®¿é—®æ¨¡å¼

2. **æ ¼å¼å½±å“**:
   - Q4_0 æ¯” Q8_0 å¿«çº¦ 2.7x (æ•°æ®é‡æ›´å°)
   - 4-bit é‡åŒ–æ˜¾è‘—å‡å°‘å†…å­˜å¸¦å®½éœ€æ±‚

3. **ç»´åº¦ç‰¹å¾**:
   - N=2 æ˜¯æç«¯æƒ…å†µï¼ˆè¾“å‡ºç»´åº¦å¾ˆå°ï¼‰
   - è¿™ç§ç»´åº¦ä¸‹ï¼Œå†…å­˜å¸¦å®½å¯èƒ½æ˜¯ç“¶é¢ˆ
   - è®¡ç®—å¼ºåº¦è¾ƒä½ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨GPU

### 3. å¯è§†åŒ–ç»“æœ

ä½¿ç”¨ Python è„šæœ¬:

```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# è¯»å–ç»“æœ
df = pd.read_csv('comparison_results.csv')

# åˆ›å»ºå¯¹æ¯”å›¾
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. æ—¶é—´å¯¹æ¯”
formats = df['Format'].unique()
llama_times = [df[(df['Implementation']=='llama.cpp') & (df['Format']==f)]['Time_us'].values[0] 
               for f in formats]
quant_times = [df[(df['Implementation']=='quant-gemm') & (df['Format']==f)]['Time_us'].values[0] 
               for f in formats]

x = np.arange(len(formats))
width = 0.35
axes[0, 0].bar(x - width/2, llama_times, width, label='llama.cpp', color='#2E86AB')
axes[0, 0].bar(x + width/2, quant_times, width, label='quant-gemm', color='#A23B72')
axes[0, 0].set_xlabel('Format')
axes[0, 0].set_ylabel('Time (Î¼s)')
axes[0, 0].set_title('Execution Time Comparison')
axes[0, 0].set_xticks(x)
axes[0, 0].set_xticklabels(formats)
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 2. TFLOPS å¯¹æ¯”
llama_tflops = [df[(df['Implementation']=='llama.cpp') & (df['Format']==f)]['TFLOPS'].values[0] 
                for f in formats]
quant_tflops = [df[(df['Implementation']=='quant-gemm') & (df['Format']==f)]['TFLOPS'].values[0] 
                for f in formats]

axes[0, 1].bar(x - width/2, llama_tflops, width, label='llama.cpp', color='#2E86AB')
axes[0, 1].bar(x + width/2, quant_tflops, width, label='quant-gemm', color='#A23B72')
axes[0, 1].set_xlabel('Format')
axes[0, 1].set_ylabel('TFLOPS')
axes[0, 1].set_title('Performance Comparison')
axes[0, 1].set_xticks(x)
axes[0, 1].set_xticklabels(formats)
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# 3. åŠ é€Ÿæ¯”
speedups = [llama_times[i] / quant_times[i] for i in range(len(formats))]
axes[1, 0].bar(formats, speedups, color='#F18F01')
axes[1, 0].axhline(y=1.0, color='r', linestyle='--', label='Baseline')
axes[1, 0].set_xlabel('Format')
axes[1, 0].set_ylabel('Speedup (llama.cpp / quant-gemm)')
axes[1, 0].set_title('Speedup Ratio')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# 4. æ•ˆç‡å¯¹æ¯”
efficiencies = [llama_tflops[i] / quant_tflops[i] for i in range(len(formats))]
axes[1, 1].bar(formats, efficiencies, color='#C73E1D')
axes[1, 1].axhline(y=1.0, color='r', linestyle='--', label='Baseline')
axes[1, 1].set_xlabel('Format')
axes[1, 1].set_ylabel('Efficiency Ratio')
axes[1, 1].set_title('Efficiency Comparison')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')
print("å›¾è¡¨å·²ä¿å­˜åˆ°: performance_comparison.png")
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: test-backend-ops æ‰¾ä¸åˆ°æµ‹è¯•

**é”™è¯¯**:
```
test-backend-ops: found 0 test(s) matching pattern
```

**è§£å†³**:
```bash
# 1. æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æµ‹è¯•
./test-backend-ops list -o MUL_MAT

# 2. ä½¿ç”¨æ›´å®½æ³›çš„æ¨¡å¼
./test-backend-ops perf -o MUL_MAT -b CUDA0 -p "q4_0"

# 3. æ£€æŸ¥æ¨¡å¼è¯­æ³•
# æ­£ç¡®: -p "type_a=q4_0.*m=4096.*n=2.*k=14336"
# é”™è¯¯: -p "q4_0 m=4096 n=2 k=14336"
```

### Q2: ç¼–è¯‘é”™è¯¯ - CUDA æ¶æ„ä¸åŒ¹é…

**é”™è¯¯**:
```
nvcc fatal: Unsupported gpu architecture 'sm_75'
```

**è§£å†³**:
```bash
# 1. æ£€æŸ¥ GPU æ¶æ„
nvidia-smi --query-gpu=compute_cap --format=csv,noheader

# 2. ä½¿ç”¨æ­£ç¡®çš„æ¶æ„
# RTX 5070 = compute capability 12.0 = sm_120
make CUDA_ARCH=sm_120
```

### Q3: æ€§èƒ½ç»“æœä¸ç¨³å®š

**ç°è±¡**: å¤šæ¬¡è¿è¡Œç»“æœå·®å¼‚å¤§

**è§£å†³**:
```bash
# 1. ç¡®ä¿ GPU ç©ºé—²
nvidia-smi

# 2. è®¾ç½® GPU æ€§èƒ½æ¨¡å¼
sudo nvidia-smi -pm 1
sudo nvidia-smi -pl 250  # è®¾ç½®åŠŸè€—é™åˆ¶ï¼ˆå¯é€‰ï¼‰

# 3. å¤šæ¬¡è¿è¡Œå–å¹³å‡
for i in {1..5}; do
    ./test-backend-ops perf -o MUL_MAT -b CUDA0 \
        -p "type_a=q4_0.*m=4096.*n=2.*k=14336" | grep "CUDA0:"
done | awk '{sum+=$2; count++} END {print "Average:", sum/count}'
```

### Q4: å†…å­˜å¯¹é½é”™è¯¯

**é”™è¯¯**:
```
CUDA error: misaligned address
```

**è§£å†³**:
- æ£€æŸ¥å†…å­˜åˆ†é…æ˜¯å¦å¯¹é½
- ä½¿ç”¨ `cudaMalloc` è€Œä¸æ˜¯ `malloc`
- æ£€æŸ¥ç»“æ„ä½“å¯¹é½ (`__align__`)

---

## ğŸ“ æµ‹è¯•æŠ¥å‘Šæ¨¡æ¿

### æµ‹è¯•æŠ¥å‘Šç¤ºä¾‹

```markdown
# é‡åŒ–GEMMæ€§èƒ½æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•ç¯å¢ƒ
- GPU: NVIDIA GeForce RTX 5070 Laptop GPU
- Compute Capability: 12.0
- CUDA Version: 12.8
- æµ‹è¯•æ—¶é—´: 2025-01-29

## æµ‹è¯•é…ç½®
- ç»´åº¦: M=4096, N=2, K=14336
- æ ¼å¼: Q4_0, Q8_0
- é‡å¤æ¬¡æ•°: 5æ¬¡å–å¹³å‡

## æµ‹è¯•ç»“æœ

### llama.cpp
| æ ¼å¼ | æ—¶é—´ (Î¼s) | TFLOPS | çŠ¶æ€ |
|------|-----------|--------|------|
| Q4_0 | 302.07 | 0.78 | âœ… PASSED |
| Q8_0 | 822.11 | 0.29 | âœ… PASSED |

### quant-gemm-from-scratch
| æ ¼å¼ | æ—¶é—´ (Î¼s) | TFLOPS | çŠ¶æ€ |
|------|-----------|--------|------|
| Q4_0 | 350.25 | 0.67 | âœ… PASSED |
| Q8_0 | 900.00 | 0.26 | âœ… PASSED |

## æ€§èƒ½åˆ†æ
- llama.cpp Q4_0 æ¯” quant-gemm å¿«çº¦ 16%
- Q4_0 æ¯” Q8_0 å¿«çº¦ 2.7x
- ä¸¤ç§å®ç°éƒ½é€šè¿‡äº†æ­£ç¡®æ€§éªŒè¯
```

---

## ğŸ¯ æœ€ä½³å®è·µ

1. **ç¯å¢ƒä¸€è‡´æ€§**: ç¡®ä¿æµ‹è¯•ç¯å¢ƒä¸€è‡´
2. **å¤šæ¬¡è¿è¡Œ**: è‡³å°‘è¿è¡Œ 3-5 æ¬¡å–å¹³å‡å€¼
3. **é¢„çƒ­**: ç¬¬ä¸€æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢ï¼Œåº”è¯¥é¢„çƒ­
4. **è®°å½•é…ç½®**: è®°å½•æ‰€æœ‰æµ‹è¯•é…ç½®
5. **å¯¹æ¯”éªŒè¯**: åŒæ—¶æµ‹è¯•ä¸¤ä¸ªå®ç°ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”
6. **æ–‡æ¡£è®°å½•**: è¯¦ç»†è®°å½•æµ‹è¯•è¿‡ç¨‹å’Œç»“æœ

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2025-01-29  
**æµ‹è¯•åœºæ™¯**: M=4096, N=2, K=14336 (FFN Up Layer)
