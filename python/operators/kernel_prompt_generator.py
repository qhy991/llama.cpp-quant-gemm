#!/usr/bin/env python3
"""
Kernel Implementation Prompt Generator

æ ¹æ® JSON schema è‡ªåŠ¨ç”Ÿæˆ kernel å®ç°æŒ‡å—ã€‚
ç”¨äºæŒ‡å¯¼ LLM æˆ–å¼€å‘è€…å®ç°ç¬¦åˆæµ‹è¯•æ¡†æ¶è¦æ±‚çš„ kernelã€‚

Usage:
    python kernel_prompt_generator.py <spec_json_path>

Example:
    python kernel_prompt_generator.py operators/quant_gemm/variants/w4a16_q4_0_fp32/spec.json
    python kernel_prompt_generator.py my_spec.json
"""

import json
import sys
from pathlib import Path
from typing import Dict, Any


# é‡åŒ–ç±»å‹å®šä¹‰
QUANTIZATION_FORMATS = {
    "block_q4_0": {
        "name": "Q4_0",
        "block_size": 32,
        "bytes_per_block": 18,
        "scale_type": "float16",
        "data_type": "uint8",
        "description": "4-bit quantization with scale. Each block of 32 values stored in 18 bytes.",
        "layout": """
Block layout (18 bytes per block, 32 values):
  Bytes 0-1:   scale (fp16)
  Bytes 2-17:  packed 4-bit values (16 bytes = 128 bits = 32 values)

Memory layout: [scale][q0][q1]...[q31] where each qi is 4 bits
""",
        "dequant_formula": "value = (q - 8) * scale",
        "dequant_code": """
// Q4_0 dequantization
__half scale = *(__half*)&block[0];  // First 2 bytes
uint8_t packed = block[i/2 + 2];     // Data starts at byte 2
uint8_t q = (i % 2 == 0) ? (packed & 0x0F) : (packed >> 4);
float value = (float(q) - 8.0f) * __half2float(scale);
"""
    },
    "block_q4_1": {
        "name": "Q4_1",
        "block_size": 32,
        "bytes_per_block": 20,
        "scale_type": "float16",
        "min_type": "float16",
        "data_type": "uint8",
        "description": "4-bit quantization with scale and min. Each block of 32 values stored in 20 bytes.",
        "layout": """
Block layout (20 bytes per block, 32 values):
  Bytes 0-1:   scale (fp16)
  Bytes 2-3:   min (fp16)
  Bytes 4-19:  packed 4-bit values (16 bytes = 128 bits = 32 values)
""",
        "dequant_formula": "value = q * scale + min",
        "dequant_code": """
// Q4_1 dequantization
__half scale = *(__half*)&block[0];
__half min = *(__half*)&block[2];
uint8_t packed = block[i/2 + 4];
uint8_t q = (i % 2 == 0) ? (packed & 0x0F) : (packed >> 4);
float value = float(q) * __half2float(scale) + __half2float(min);
"""
    },
    "block_q8_0": {
        "name": "Q8_0",
        "block_size": 32,
        "bytes_per_block": 34,
        "scale_type": "float16",
        "data_type": "int8",
        "description": "8-bit quantization with scale. Each block of 32 values stored in 34 bytes.",
        "layout": """
Block layout (34 bytes per block, 32 values):
  Bytes 0-1:   scale (fp16)
  Bytes 2-33:  int8 values (32 bytes)
""",
        "dequant_formula": "value = q * scale",
        "dequant_code": """
// Q8_0 dequantization
__half scale = *(__half*)&block[0];
int8_t q = block[i + 2];
float value = float(q) * __half2float(scale);
"""
    },
    "block_q8_1": {
        "name": "Q8_1",
        "block_size": 32,
        "bytes_per_block": 36,
        "scale_type": "float16",
        "min_type": "float16",
        "data_type": "int8",
        "description": "8-bit quantization with scale and min. Each block of 32 values stored in 36 bytes.",
        "layout": """
Block layout (36 bytes per block, 32 values):
  Bytes 0-1:   scale (fp16)
  Bytes 2-3:   min (fp16)
  Bytes 4-35:  int8 values (32 bytes)
""",
        "dequant_formula": "value = q * scale + min",
        "dequant_code": """
// Q8_1 dequantization
__half scale = *(__half*)&block[0];
__half min = *(__half*)&block[2];
int8_t q = block[i + 4];
float value = float(q) * __half2float(scale) + __half2float(min);
"""
    },
    "float32": {
        "name": "FP32",
        "description": "Standard 32-bit floating point",
        "layout": "Standard FP32 (4 bytes per value)",
        "dequant_formula": "value = x",  # No quantization
        "dequant_code": "// No dequantization needed for FP32\nfloat value = x;"
    }
}


def format_shape(shape, params: Dict[str, Any]) -> str:
    """å°† shape è¡¨è¾¾å¼è½¬æ¢ä¸ºå¯è¯»æ ¼å¼"""
    context = params.copy()
    # è¯„ä¼° K/32 è¿™æ ·çš„è¡¨è¾¾å¼
    result = []
    for dim in shape:
        dim_str = str(dim)
        if "/" in dim_str:
            parts = dim_str.split("/")
            try:
                val = int(context[parts[0]]) // int(parts[1])
                result.append(f"{val} (computed as {dim})")
            except:
                result.append(dim_str)
        else:
            val = context.get(dim_str, dim)
            result.append(str(val))
    return f"[{', '.join(result)}]"


def generate_input_description(input_name: str, input_spec: Dict[str, Any],
                               quant_info: Dict, params: Dict[str, Any]) -> str:
    """ç”Ÿæˆå•ä¸ªè¾“å…¥çš„æè¿°"""
    dtype = input_spec["dtype"]
    shape = input_spec["shape"]

    lines = [
        f"### {input_name.title()}",
        "",
        f"**Data Type:** `{dtype}`",
        ""
    ]

    # æ·»åŠ é‡åŒ–æ ¼å¼ä¿¡æ¯
    if dtype in QUANTIZATION_FORMATS:
        qinfo = QUANTIZATION_FORMATS[dtype]
        lines.extend([
            f"**Format:** {qinfo['name']}",
            "",
            qinfo["description"],
            "",
            "**Memory Layout:**",
            "```",
            qinfo["layout"],
            "```",
            "",
            "**Dequantization Formula:**",
            f"```cpp",
            f"{qinfo['dequant_formula']}",
            "```",
            "",
            "**Dequantization Code:**",
            "```cpp",
            qinfo["dequant_code"],
            "```"
        ])

    lines.extend([
        "",
        f"**Shape:** `{format_shape(shape, params)}`",
        "",
        input_spec.get("description", ""),
        ""
    ])

    return "\n".join(lines)


def generate_kernel_signature(spec: Dict[str, Any]) -> str:
    """ç”Ÿæˆ kernel å‡½æ•°ç­¾å"""
    kernel = spec["kernel"]
    entry_point = kernel["entry_point"]
    inputs = spec["inputs"]
    outputs = spec["outputs"]

    # æ”¶é›†å‚æ•°
    params_list = []

    # è¾“å…¥å‚æ•°
    for name, input_spec in inputs.items():
        dtype = input_spec["dtype"]
        shape = input_spec["shape"]
        shape_str = ", ".join(str(s) for s in shape)

        # ç¡®å®šæŒ‡é’ˆç±»å‹
        if dtype == "float32":
            params_list.append(f"const float* {name}")
        elif dtype in QUANTIZATION_FORMATS:
            params_list.append(f"const uint8_t* {name}")
        else:
            params_list.append(f"const void* {name}")

    # è¾“å‡ºå‚æ•°
    for name, output_spec in outputs.items():
        dtype = output_spec["dtype"]
        if dtype == "float32":
            params_list.append(f"float* {name}")
        else:
            params_list.append(f"void* {name}")

    # ç»´åº¦å‚æ•°
    params_list.extend(["int M", "int N", "int K"])

    signature = f"__global__ void {entry_point}(\n    "
    signature += ",\n    ".join(params_list)
    signature += "\n)"

    return signature


def generate_math_formula(spec: Dict[str, Any]) -> str:
    """ç”Ÿæˆæ•°å­¦å…¬å¼"""
    lines = [
        "## æ•°å­¦å…¬å¼",
        "",
        "### é«˜å±‚å…¬å¼",
        "```"
    ]

    formula = spec.get("formula", {})
    outputs = spec["outputs"]
    inputs = spec["inputs"]

    output_shape = outputs["output"]["shape"]
    input_names = list(inputs.keys())

    if "gemm" in formula:
        lines.append(formula["gemm"])
    elif "dot_product" in formula:
        lines.append(formula["dot_product"])
    else:
        # é»˜è®¤ GEMM å…¬å¼
        lines.append(f"C[m,n] = sum_k({input_names[0]}[n,k] * {input_names[1]}[m,k])")

    lines.append("```")
    lines.append("")

    # æ·»åŠ è§£é‡Š
    if "explanation" in formula:
        lines.extend([
            "**è§£é‡Š:**",
            "",
            formula["explanation"],
            ""
        ])

    # å¦‚æœæœ‰åé‡åŒ–å…¬å¼
    if "dequantize" in formula:
        lines.extend([
            "### åé‡åŒ–",
            "```cpp",
            formula["dequantize"],
            "```"
        ])

    return "\n".join(lines)


def generate_pybind_section(spec: Dict[str, Any]) -> str:
    """ç”Ÿæˆ pybind11 é›†æˆè¯´æ˜"""
    kernel = spec["kernel"]
    entry_point = kernel["entry_point"]
    name = spec["name"]

    # ç”Ÿæˆå‚æ•°ç±»å‹
    params = []
    for input_name, input_spec in spec["inputs"].items():
        dtype = input_spec["dtype"]
        if dtype == "float32":
            params.append("py::array_t<float>")
        else:
            params.append("py::array_t<uint8_t>")

    for output_name, output_spec in spec["outputs"].items():
        dtype = output_spec["dtype"]
        if dtype == "float32":
            params.append("py::array_t<float>")
        else:
            params.append("py::array_t<uint8_t>")

    params_str = ", ".join(params)

    return f"""## Pybind11 é›†æˆ

åœ¨ `bindings.cpp` ä¸­æ·»åŠ ä»¥ä¸‹å£°æ˜:

```cpp
// Include header (if separate)
// #include "kernels/{kernel['file']}"

// Binding declaration
m.def("{entry_point}",
    []({params_str}) {{
        // TODO: Implement buffer_info extraction and kernel launch
        // See w4a8_q4_0_q8_1 variant for reference
        py::gil_scoped_release release;
        // Launch kernel here
    }},
    py::arg({", ".join([f'"{n}"' for n in spec["inputs"].keys()])}),
    "Kernel implementation for {name}"
);
```

**é‡è¦æç¤º:**
- å‡½æ•°åå¿…é¡»ä¸ `spec.json` ä¸­çš„ `kernel.entry_point` ä¸€è‡´
- å‚æ•°é¡ºåºå¿…é¡»ä¸ spec ä¸­å®šä¹‰çš„é¡ºåºä¸€è‡´
- å¿…é¡»é‡Šæ”¾ GIL (`py::gil_scoped_release`)
"""


def generate_test_section(spec: Dict[str, Any]) -> str:
    """ç”Ÿæˆæµ‹è¯•éªŒè¯è¯´æ˜"""
    accuracy = spec.get("accuracy", {})
    test_configs = spec.get("test_configs", [])
    reference = spec.get("reference", "")

    metric = accuracy.get("metric", "nmse")
    threshold = accuracy.get("threshold", 0.05)

    lines = [
        "## æµ‹è¯•æ¡†æ¶éªŒè¯",
        "",
        "### éªŒè¯æµç¨‹",
        "",
        "```",
        "1. æµ‹è¯•æ¡†æ¶ç”Ÿæˆéšæœºè¾“å…¥æ•°æ®",
        "2. è°ƒç”¨ reference.py ç”Ÿæˆæ­£ç¡®è¾“å‡º",
        "3. è°ƒç”¨ä½ çš„ kernel ç”Ÿæˆå®é™…è¾“å‡º",
        "4. æ¯”è¾ƒä¸¤è€…å¹¶è®¡ç®— " + metric.upper(),
        "5. éªŒè¯ " + metric.upper() + f" æ˜¯å¦ â‰¤ {threshold}",
        "```",
        "",
        "### ç²¾åº¦è¦æ±‚",
        "",
        f"- **æŒ‡æ ‡:** {metric.upper()}",
        f"- **é˜ˆå€¼:** {threshold}",
        "",
        f"**NMSE è®¡ç®—å…¬å¼:**",
        "```python",
        "nmse = np.mean((ref - actual) ** 2) / np.mean(ref ** 2)",
        "```",
        "",
        "### æµ‹è¯•é…ç½®",
        ""
    ]

    for config in test_configs:
        lines.append(f"- `{config['name']}`: M={config['M']}, N={config['N']}, K={config['K']}")

    lines.extend([
        "",
        "### å‚è€ƒå®ç°",
        "",
        f"ä½ç½®: `{reference}`",
        "",
        "### éªŒæ”¶æ ‡å‡†",
        "",
        "1. **æ­£ç¡®æ€§**: æ‰€æœ‰æµ‹è¯•é…ç½®çš„ NMSE â‰¤ " + str(threshold),
        "2. **æ€§èƒ½**: éœ€è¦è¾¾åˆ°æœ€ä½æ€§èƒ½ç›®æ ‡",
        "3. **ç¨³å®šæ€§**: å¤šæ¬¡è¿è¡Œç»“æœä¸€è‡´",
        ""
    ])

    return "\n".join(lines)


def generate_common_pitfalls_section(spec: Dict[str, Any]) -> str:
    """ç”Ÿæˆå¸¸è§é”™è¯¯è­¦å‘Šéƒ¨åˆ†"""
    inputs = spec["inputs"]

    # æ£€æµ‹ä½¿ç”¨çš„é‡åŒ–æ ¼å¼
    used_formats = set()
    has_fp32_activation = False
    has_quant_activation = False

    for input_name, input_spec in inputs.items():
        dtype = input_spec["dtype"]
        if dtype in QUANTIZATION_FORMATS and dtype != "float32":
            used_formats.add(dtype)
            if "activation" in input_name.lower():
                has_quant_activation = True
        elif dtype == "float32" and "activation" in input_name.lower():
            has_fp32_activation = True

    lines = [
        "## âš ï¸ å¸¸è§é”™è¯¯å’Œé™·é˜±",
        "",
        "**è¯·ä»”ç»†é˜…è¯»æœ¬èŠ‚ä»¥é¿å…å¸¸è§çš„å®ç°é”™è¯¯ï¼**",
        "",
    ]

    # Q4_0 ç‰¹å®šè­¦å‘Š
    if "block_q4_0" in used_formats:
        lines.extend([
            "### ğŸš¨ CRITICAL: Q4_0 Packing Format",
            "",
            "**Q4_0 ä½¿ç”¨ SPLIT-BY-16 æ‰“åŒ…ï¼Œä¸æ˜¯è¿ç»­å¯¹ï¼**",
            "",
            "âœ… **æ­£ç¡®çš„ç†è§£:**",
            "```",
            "byte[0]  = weight[0]  (low nibble) | weight[16] (high nibble)",
            "byte[1]  = weight[1]  (low nibble) | weight[17] (high nibble)",
            "...",
            "byte[15] = weight[15] (low nibble) | weight[31] (high nibble)",
            "```",
            "",
            "âŒ **é”™è¯¯çš„ç†è§£ (å¸¸è§é”™è¯¯):**",
            "```",
            "byte[0] = weight[0] (low) | weight[1] (high)  // WRONG!",
            "byte[1] = weight[2] (low) | weight[3] (high)  // WRONG!",
            "```",
            "",
            "âœ… **æ­£ç¡®çš„è§£åŒ…ä»£ç :**",
            "```cpp",
            "for (int i = 0; i < 16; i++) {",
            "    uint8_t packed = data_ptr[i];",
            "    ",
            "    // Low nibble -> weight[i]",
            "    uint8_t q0 = packed & 0x0F;",
            "    float w0 = (float(q0) - 8.0f) * scale;",
            "    sum += activation[k_start + i] * w0;",
            "    ",
            "    // High nibble -> weight[i + 16]",
            "    uint8_t q1 = packed >> 4;",
            "    float w1 = (float(q1) - 8.0f) * scale;",
            "    sum += activation[k_start + i + 16] * w1;",
            "}",
            "```",
            "",
            "**éªŒè¯æ–¹æ³•:**",
            "1. å…ˆæµ‹è¯• quantize -> dequantize å¾€è¿”",
            "2. ä½¿ç”¨ç®€å•å›ºå®šå€¼æµ‹è¯• (weight=0.5, activation=2.0)",
            "3. ç¡®ä¿ NMSE < 0.05",
            "",
        ])

    # ç»´åº¦çº¦å®šè­¦å‘Š
    lines.extend([
        "### ğŸš¨ Dimension Conventions",
        "",
    ])

    if has_quant_activation:
        lines.extend([
            "**æœ¬ kernel ä½¿ç”¨é‡åŒ– activation (w4a8 çº¦å®š):**",
            "",
            "```cpp",
            "// Kernel è®¡ç®—: C[N, M] = W[N, K] @ A[M, K]^T",
            "// è°ƒç”¨çº¦å®š: kernel(weight, activation, N, M, K)",
            "// è¾“å‡ºéœ€è¦è½¬ç½®: output.T å¾—åˆ° [M, N]",
            "```",
            "",
        ])
    elif has_fp32_activation:
        lines.extend([
            "**æœ¬ kernel ä½¿ç”¨ FP32 activation (w4a16 çº¦å®š):**",
            "",
            "```cpp",
            "// Kernel è®¡ç®—: C[M, N] = A[M, K] @ W[N, K]^T",
            "// è°ƒç”¨çº¦å®š: kernel(weight, activation, M, N, K)",
            "// è¾“å‡ºç›´æ¥æ˜¯ [M, N]ï¼Œæ— éœ€è½¬ç½®",
            "```",
            "",
        ])

    lines.extend([
        "### ğŸš¨ Memory and Performance Pitfalls",
        "",
        "1. **Integer Overflow**",
        "   ```cpp",
        "   // âŒ WRONG: å¯èƒ½æº¢å‡º",
        "   int offset = n * num_blocks * 18;",
        "   ",
        "   // âœ… CORRECT: ä½¿ç”¨ long long",
        "   long long offset = (long long)(n * num_blocks) * 18;",
        "   ```",
        "",
        "2. **Memory Alignment (float4)**",
        "   ```cpp",
        "   // float4 éœ€è¦ 16-byte å¯¹é½",
        "   // å¦‚æœåœ°å€æœªå¯¹é½ï¼Œä¼šé€€åŒ–ä¸º 4 æ¬¡å•ç‹¬è¯»å–",
        "   // ç¡®ä¿ K æ˜¯ 4 çš„å€æ•°ä¸”èµ·å§‹åœ°å€å¯¹é½",
        "   ```",
        "",
        "3. **Quantization Offset**",
        "   ```cpp",
        "   // Q4_0: å€¼åç§» 8 (èŒƒå›´ [0,15] -> [-8,7])",
        "   float w = (float(q) - 8.0f) * scale;  // å¿…é¡»å‡ 8!",
        "   ```",
        "",
        "4. **Block Size Assumptions**",
        "   ```cpp",
        "   // ä¸è¦ç¡¬ç¼–ç  32ï¼Œä½¿ç”¨å¸¸é‡",
        "   int num_blocks = K / QK4_0;  // QK4_0 = 32",
        "   ```",
        "",
    ])

    lines.extend([
        "### âœ… Testing Best Practices",
        "",
        "**æµ‹è¯•é¡ºåº (ä»ç®€å•åˆ°å¤æ‚):**",
        "",
        "1. **Quantization Roundtrip**",
        "   ```python",
        "   x -> quantize -> dequantize -> x'",
        "   max_error = (x - x').abs().max()",
        "   assert max_error < 1.0  # Q4_0 æœ‰æ˜¾è‘—è¯¯å·®",
        "   ```",
        "",
        "2. **Fixed Values**",
        "   ```python",
        "   weight = torch.full((N, K), 0.5)",
        "   activation = torch.full((M, K), 2.0)",
        "   expected_output = K * 0.5 * 2.0 = K",
        "   ```",
        "",
        "3. **Different Data Patterns**",
        "   - All zeros",
        "   - All ones",
        "   - Positive only (torch.rand)",
        "   - Mixed signs (torch.randn) â† æœ€å®¹æ˜“æš´éœ² bug",
        "",
        "4. **NMSE Thresholds**",
        "   - Q4_0: NMSE < 0.05 (5%)",
        "   - Q8_1: NMSE < 0.01 (1%)",
        "   - FP16: NMSE < 0.001 (0.1%)",
        "",
    ])

    lines.extend([
        "### ğŸ“š Reference Implementations",
        "",
        "**åœ¨å®ç°å‰ï¼Œè¯·å‚è€ƒ:**",
        "",
        "1. **Dequantization Reference**",
        "   - æŸ¥çœ‹ `dequantize_q4_0_kernel` in gemm_ops.cu",
        "   - ç¡®ä¿ä½ çš„è§£åŒ…é€»è¾‘ä¸ä¹‹ä¸€è‡´",
        "",
        "2. **llama.cpp Q4_0 Format**",
        "   - https://github.com/ggerganov/llama.cpp/blob/master/ggml.c",
        "   - æœç´¢ `dequantize_row_q4_0`",
        "",
        "3. **Working Kernels**",
        "   - w4a8_q4_0_q8_1: å‚è€ƒé‡åŒ– activation çš„å®ç°",
        "   - w4a16_q4_0_fp32: å‚è€ƒ FP32 activation çš„å®ç°",
        "",
    ])

    return "\n".join(lines)


def generate_implementation_checklist(spec: Dict[str, Any]) -> str:
    """ç”Ÿæˆå®ç°æ£€æŸ¥æ¸…å•"""
    name = spec["name"]
    entry_point = spec["kernel"]["entry_point"]

    inputs_desc = []
    for input_name, input_spec in spec["inputs"].items():
        dtype = input_spec["dtype"]
        shape = input_spec["shape"]
        inputs_desc.append(f"  - {input_name}: {dtype}, shape={'x'.join(str(s) for s in shape)}")

    outputs_desc = []
    for output_name, output_spec in spec["outputs"].items():
        dtype = output_spec["dtype"]
        shape = output_spec["shape"]
        outputs_desc.append(f"  - {output_name}: {dtype}, shape={'x'.join(str(s) for s in shape)}")

    accuracy = spec.get("accuracy", {})
    threshold = accuracy.get("threshold", 0.05)

    return f"""## å®ç°æ£€æŸ¥æ¸…å•

### å¼€å§‹å®ç°å‰

- [ ] é˜…è¯» `KERNEL_IMPLEMENTATION_GUIDE.md` äº†è§£ GEMM åŸºç¡€
- [ ] ç†è§£æœ¬æŒ‡å—ä¸­çš„æ‰€æœ‰è¾“å…¥æ ¼å¼å’Œæ•°å­¦å…¬å¼
- [ ] é˜…è¯» `{spec['kernel']['file']}` ä¸­çš„å‚è€ƒå®ç°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰

### å®ç°æ­¥éª¤

1. [ ] **åˆ›å»ºæ–‡ä»¶**: `operators/quant_gemm/variants/{name}/{spec['kernel']['file']}`
2. [ ] **å®ç° kernel å‡½æ•°**:
   ```cpp
   {generate_kernel_signature(spec)}
   {{
       // TODO: å®ç° kernel é€»è¾‘
   }}
   ```
3. [ ] **æ·»åŠ  pybind11 å£°æ˜**: åœ¨ `bindings.cpp` ä¸­æ³¨å†Œå‡½æ•°
4. [ ] **ç¼–è¯‘éªŒè¯**: `python setup.py build_ext --inplace`
5. [ ] **è¿è¡Œæµ‹è¯•**: `python test_operator.py {name} operators/quant_gemm/variants/{name}`

### è¾“å…¥å‚æ•°

{chr(10).join(inputs_desc)}

### è¾“å‡ºå‚æ•°

{chr(10).join(outputs_desc)}

### éªŒæ”¶æ ‡å‡†

- [ ] æ‰€æœ‰æµ‹è¯•é…ç½®é€šè¿‡
- [ ] NMSE â‰¤ {threshold}
- [ ] æ— å†…å­˜æ³„æ¼
- [ ] ä»£ç ç¬¦åˆé¡¹ç›®è§„èŒƒ
"""


def generate_prompt(spec: Dict[str, Any], variant_path: Path) -> str:
    """ç”Ÿæˆå®Œæ•´çš„ kernel å®ç°æç¤º"""
    name = spec["name"]
    description = spec.get("description", "")
    family = spec.get("family", "")
    version = spec.get("version", "1.0.0")

    # è·å–é»˜è®¤å‚æ•°
    default_params = {
        "M": 1,
        "N": spec["params"]["N"].get("default", 4096),
        "K": spec["params"]["K"].get("default", 4096)
    }

    lines = [
        f"# Kernel å®ç°æŒ‡å—: {name}",
        "",
        f"**Family:** {family}",
        f"**Version:** {version}",
        "",
        description,
        "",
        "---",
        "",
        "## æ¦‚è¿°",
        "",
        f"æœ¬æŒ‡å—æè¿°å¦‚ä½•å®ç° `{name}` kernelã€‚è¯¥ kernel æ˜¯é‡åŒ–çŸ©é˜µä¹˜æ³•(GEMM)çš„ä¸€ä¸ªå˜ä½“ã€‚",
        "",
        "## ç›®å½•",
        "",
        "1. [Kernel å‡½æ•°ç­¾å](#kernel-å‡½æ•°ç­¾å)",
        "2. [è¾“å…¥æ ¼å¼](#è¾“å…¥æ ¼å¼)",
        "3. [è¾“å‡ºæ ¼å¼](#è¾“å‡ºæ ¼å¼)",
        "4. [æ•°å­¦å…¬å¼](#æ•°å­¦å…¬å¼)",
        "5. [Pybind11 é›†æˆ](#pybind11-é›†æˆ)",
        "6. [æµ‹è¯•éªŒè¯](#æµ‹è¯•éªŒè¯)",
        "7. [å®ç°æ£€æŸ¥æ¸…å•](#å®ç°æ£€æŸ¥æ¸…å•)",
        "",
        "---",
        "",
        "## Kernel å‡½æ•°ç­¾å",
        "",
        "### å¿…é¡»ä½¿ç”¨çš„å‡½æ•°åå’Œç­¾å",
        "",
        "```cpp",
        generate_kernel_signature(spec),
        "```",
        "",
        "**å‚æ•°è¯´æ˜:**",
        ""
    ]

    # æ·»åŠ å‚æ•°è¯´æ˜
    for param_name, param_spec in spec["params"].items():
        default = param_spec.get("default", "N/A")
        constraint = param_spec.get("constraint", "æ— ")
        lines.extend([
            f"- **{param_name}**: {param_spec['description']}",
            f"  - é»˜è®¤å€¼: {default}",
            f"  - çº¦æŸ: {constraint}",
            ""
        ])

    lines.extend([
        "",
        "## è¾“å…¥æ ¼å¼",
        ""
    ])

    # æ·»åŠ è¾“å…¥æè¿°
    for input_name, input_spec in spec["inputs"].items():
        dtype = input_spec["dtype"]
        quant_info = QUANTIZATION_FORMATS.get(dtype, {})
        lines.append(generate_input_description(input_name, input_spec, quant_info, default_params))
        lines.append("")

    lines.extend([
        "### å†…å­˜å¸ƒå±€çº¦å®š",
        "",
        "- æ‰€æœ‰è¾“å…¥éƒ½æ˜¯è¡Œä¼˜å…ˆ (row-major) å­˜å‚¨",
        "- K ç»´åº¦å¿…é¡»æ˜¯ 32 çš„å€æ•° (é‡åŒ– block size)",
        "- å¯¹äºé‡åŒ–æ•°æ®ï¼Œæœ€åä¸€ä¸ªç»´åº¦åŒ…å«å®Œæ•´çš„ block å­—èŠ‚æ•°",
        "",
        "---",
        "",
        "## è¾“å‡ºæ ¼å¼",
        ""
    ])

    # æ·»åŠ è¾“å‡ºæè¿°
    for output_name, output_spec in spec["outputs"].items():
        dtype = output_spec["dtype"]
        shape = output_spec["shape"]
        lines.extend([
            f"### {output_name.title()}",
            "",
            f"**Data Type:** `{dtype}`",
            f"**Shape:** `{'x'.join(shape)}`",
            "",
            output_spec.get("description", ""),
            ""
        ])

    lines.extend([
        "**æ³¨æ„:** è¾“å‡ºæ˜¯è¡Œä¼˜å…ˆå­˜å‚¨çš„ [M, N] çŸ©é˜µã€‚",
        "",
        generate_math_formula(spec),
        "",
        "---",
        "",
        generate_common_pitfalls_section(spec),
        "",
        "---",
        "",
        generate_pybind_section(spec),
        "",
        "---",
        "",
        generate_test_section(spec),
        "",
        "---",
        "",
        generate_implementation_checklist(spec),
        "",
        "---",
        "",
        "## å‚è€ƒèµ„æº",
        "",
        f"- **Variant ç›®å½•:** `{variant_path}`",
        f"- **Spec æ–‡ä»¶:** `{variant_path}/spec.json`",
        f"- **Kernel æ–‡ä»¶:** `{variant_path}/{spec['kernel']['file']}`",
        f"- **Reference å®ç°:** `{variant_path}/{spec.get('reference', 'reference.py')}`",
        "",
        "---",
        "",
        "## å¿«é€Ÿå¼€å§‹",
        "",
        f"```bash",
        f"# 1. åˆ›å»º kernel æ–‡ä»¶",
        f"touch operators/quant_gemm/variants/{name}/{spec['kernel']['file']}",
        f"",
        f"# 2. å®ç° kernel (å‚è€ƒä¸Šé¢çš„å‡½æ•°ç­¾å)",
        f"",
        f"# 3. åœ¨ bindings.cpp ä¸­æ·»åŠ å£°æ˜",
        f"",
        f"# 4. ç¼–è¯‘",
        f"python setup.py build_ext --inplace",
        f"",
        f"# 5. æµ‹è¯•",
        f"python test_operator.py {name} operators/quant_gemm/variants/{name}",
        f"```"
    ])

    return "\n".join(lines)


def main():
    if len(sys.argv) < 2:
        print("Usage: python kernel_prompt_generator.py <spec_json_path>")
        print("")
        print("Examples:")
        print("  python kernel_prompt_generator.py operators/quant_gemm/variants/w4a16_q4_0_fp32/spec.json")
        print("  python kernel_prompt_generator.py my_spec.json")
        sys.exit(1)

    spec_file = Path(sys.argv[1])

    if not spec_file.exists():
        print(f"Error: spec file not found: {spec_file}")
        sys.exit(1)

    variant_path = spec_file.parent

    with open(spec_file, 'r') as f:
        spec = json.load(f)

    prompt = generate_prompt(spec, variant_path)

    # è¾“å‡ºåˆ°æ–‡ä»¶
    output_file = variant_path / "IMPLEMENTATION_PROMPT.md"
    with open(output_file, 'w') as f:
        f.write(prompt)

    print(f"Generated implementation prompt: {output_file}")
    print("")
    print(f"To view: cat {output_file}")
    print(f"Or use with LLM: cat {output_file} | llm ...")


if __name__ == "__main__":
    main()
